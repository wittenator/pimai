{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fd1945420f0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from torch.distributions import *\n",
    "\n",
    "import skorch\n",
    "import numpy as np\n",
    "\n",
    "%load_ext tensorboard\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': cpu_count(), 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = DataLoader(Subset(\n",
    "    datasets.MNIST('/data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])), indices=range(10000)),\n",
    "    batch_size=64, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(Subset(\n",
    "    datasets.MNIST('/data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])), indices=range(10000)),\n",
    "    batch_size=1000, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.writer = SummaryWriter(log_dir='/data/runs')\n",
    "    \n",
    "    def trains(self, device, train_loader, optimizer, epoch):\n",
    "        self.train()\n",
    "        loss_sum = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.compute_loss_train(data, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "            self.writer.add_scalar('Loss/train', loss.item(), epoch*len(train_loader)+batch_idx)\n",
    "            \n",
    "    def tests(self, device, test_loader):\n",
    "        self.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                loss, output = self.compute_loss_test(data, target)\n",
    "                test_loss += loss\n",
    "                l1 = F.l1_loss(output, data.view(-1, 784), reduction='sum')\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}, Reconstruction error: {}\\n'.format(\n",
    "            test_loss, l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoencoder(Autoencoder):\n",
    "    def __init__(self):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output       \n",
    "    \n",
    "    def compute_loss_train(self, data, target):\n",
    "        output = self(data)\n",
    "        return F.nll_loss(output, target)\n",
    "    \n",
    "    def compute_loss_test(self, data, target):\n",
    "        output = self(data)\n",
    "        return F.nll_loss(output, target, reduction='sum').item(), output  # sum up batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.325908\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-2eebc719d3ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-db8de939b0be>\u001b[0m in \u001b[0;36mtrains\u001b[0;34m(self, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-2b21ab3cdbc4>\u001b[0m in \u001b[0;36mcompute_loss_train\u001b[0;34m(self, data, target)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-2b21ab3cdbc4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SimpleAutoencoder().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters())\n",
    "\n",
    "# plot model\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# write to tensorboard\n",
    "#writer.add_image('mnist_images', img_grid)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "for epoch in range(1, 14 + 1):\n",
    "    model.trains(device, train_loader, optimizer, epoch)\n",
    "    model.tests(device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Autoencoder):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return BCE + KLD\n",
    "    \n",
    "    def compute_loss_train(self, data, target):\n",
    "        recon_batch, mu, logvar = self(data)\n",
    "        return self.loss_function(recon_batch, data, mu, logvar)\n",
    "    \n",
    "    def compute_loss_test(self, data, target):\n",
    "        recon_batch, mu, logvar = self(data)\n",
    "        return self.loss_function(recon_batch, data, mu, logvar).item(), recon_batch  # sum up batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 35111.679688\n",
      "Train Epoch: 1 [640/10000 (6%)]\tLoss: -100550.734375\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: -290526.843750\n",
      "Train Epoch: 1 [1920/10000 (19%)]\tLoss: -367270.281250\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: -398255.500000\n",
      "Train Epoch: 1 [3200/10000 (32%)]\tLoss: -438068.968750\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: -512162.750000\n",
      "Train Epoch: 1 [4480/10000 (45%)]\tLoss: -534456.562500\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: -564552.687500\n",
      "Train Epoch: 1 [5760/10000 (57%)]\tLoss: -667109.625000\n",
      "Train Epoch: 1 [6400/10000 (64%)]\tLoss: -716329.500000\n",
      "Train Epoch: 1 [7040/10000 (70%)]\tLoss: -738120.000000\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: -718507.750000\n",
      "Train Epoch: 1 [8320/10000 (83%)]\tLoss: -751547.312500\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: -861383.875000\n",
      "Train Epoch: 1 [9600/10000 (96%)]\tLoss: -846808.000000\n",
      "\n",
      "Test set: Average loss: -13556.3616, Reconstruction error: 495748.84375\n",
      "\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: -854750.750000\n",
      "Train Epoch: 2 [640/10000 (6%)]\tLoss: -864394.750000\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: -833788.875000\n",
      "Train Epoch: 2 [1920/10000 (19%)]\tLoss: -858662.875000\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: -800703.937500\n",
      "Train Epoch: 2 [3200/10000 (32%)]\tLoss: -887240.500000\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: -831678.437500\n",
      "Train Epoch: 2 [4480/10000 (45%)]\tLoss: -872596.187500\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: -921771.375000\n",
      "Train Epoch: 2 [5760/10000 (57%)]\tLoss: -853350.500000\n",
      "Train Epoch: 2 [6400/10000 (64%)]\tLoss: -885011.375000\n",
      "Train Epoch: 2 [7040/10000 (70%)]\tLoss: -868209.000000\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: -911518.875000\n",
      "Train Epoch: 2 [8320/10000 (83%)]\tLoss: -834833.500000\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: -887044.062500\n",
      "Train Epoch: 2 [9600/10000 (96%)]\tLoss: -890720.937500\n",
      "\n",
      "Test set: Average loss: -14095.1669, Reconstruction error: 491923.34375\n",
      "\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: -881186.187500\n",
      "Train Epoch: 3 [640/10000 (6%)]\tLoss: -913567.812500\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: -874127.500000\n",
      "Train Epoch: 3 [1920/10000 (19%)]\tLoss: -897517.687500\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: -888979.875000\n",
      "Train Epoch: 3 [3200/10000 (32%)]\tLoss: -888603.187500\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: -922794.125000\n",
      "Train Epoch: 3 [4480/10000 (45%)]\tLoss: -885454.812500\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: -905358.500000\n",
      "Train Epoch: 3 [5760/10000 (57%)]\tLoss: -888017.375000\n",
      "Train Epoch: 3 [6400/10000 (64%)]\tLoss: -859465.000000\n",
      "Train Epoch: 3 [7040/10000 (70%)]\tLoss: -897193.687500\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: -914457.437500\n",
      "Train Epoch: 3 [8320/10000 (83%)]\tLoss: -915454.250000\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: -836272.937500\n",
      "Train Epoch: 3 [9600/10000 (96%)]\tLoss: -905532.250000\n",
      "\n",
      "Test set: Average loss: -14156.5123, Reconstruction error: 488775.40625\n",
      "\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: -872309.187500\n",
      "Train Epoch: 4 [640/10000 (6%)]\tLoss: -871135.875000\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: -937939.000000\n",
      "Train Epoch: 4 [1920/10000 (19%)]\tLoss: -916547.437500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-e0fab7e61e65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-a038974cc7f4>\u001b[0m in \u001b[0;36mtrains\u001b[0;34m(self, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-eceeab2d83eb>\u001b[0m in \u001b[0;36mcompute_loss_train\u001b[0;34m(self, data, target)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-eceeab2d83eb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-eceeab2d83eb>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "for epoch in range(1, 14 + 1):\n",
    "    model.trains(device, train_loader, optimizer, epoch)\n",
    "    model.tests(device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stick-breaking process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stickbreakingprocess(k, a, b):\n",
    "    batch_size = a.size()[0]\n",
    "    uniform_samples = Uniform(torch.tensor([0.0]), torch.tensor([1.0])).rsample(torch.tensor([batch_size,k])).view(-1,k)\n",
    "    exp_a = 1/a\n",
    "    exp_b = 1/b\n",
    "    km = (1- uniform_samples.pow(exp_b)).pow(exp_a)\n",
    "    \n",
    "    #no Nans are allowed in the matrix\n",
    "    #assert not torch.isnan(km).any().item()\n",
    "    \n",
    "    sticks = torch.zeros(batch_size,k)\n",
    "    remaining_sticks = torch.ones_like(km[:,0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,k-1):\n",
    "            sticks[:,i] = remaining_sticks * km[:,i]\n",
    "            remaining_sticks *= (1-km[:,i])\n",
    "        sticks[:,k-1] = remaining_sticks\n",
    "    latent_variables = sticks\n",
    "\n",
    "    #all stick segments must sum to 1\n",
    "    #assert torch.allclose(latent_variables.sum(axis=1), torch.ones([batch_size]))\n",
    "    \n",
    "    return latent_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stickbreakingprocess(20, torch.rand(10,20), torch.rand(10,20)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2840, 0.0518],\n",
       "         [0.7416, 0.3510],\n",
       "         [0.0801, 0.8012],\n",
       "         [0.3975, 0.7871],\n",
       "         [0.9122, 0.6335]],\n",
       "\n",
       "        [[0.7045, 0.6593],\n",
       "         [0.6110, 0.3700],\n",
       "         [0.1818, 0.2044],\n",
       "         [0.3644, 0.4365],\n",
       "         [0.9949, 0.6306]],\n",
       "\n",
       "        [[0.4014, 0.6926],\n",
       "         [0.9259, 0.5556],\n",
       "         [0.2471, 0.2153],\n",
       "         [0.0361, 0.9757],\n",
       "         [0.1972, 0.3511]],\n",
       "\n",
       "        [[0.7728, 0.4319],\n",
       "         [0.5881, 0.0910],\n",
       "         [0.4601, 0.1714],\n",
       "         [0.9667, 0.2084],\n",
       "         [0.9598, 0.7851]],\n",
       "\n",
       "        [[0.2760, 0.4266],\n",
       "         [0.4791, 0.1536],\n",
       "         [0.9422, 0.8401],\n",
       "         [0.3884, 0.9252],\n",
       "         [0.1256, 0.6673]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand([5,10])\n",
    "t.unfold(1,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stick-breaking Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBVAE(Autoencoder):\n",
    "    def __init__(self, k):\n",
    "        super(SBVAE, self).__init__()\n",
    "        self.k = k\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 400)\n",
    "        self.fc21 = nn.Linear(400, k)\n",
    "        self.fc22 = nn.Linear(400, k)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return F.softplus(self.fc21(h1)), F.softplus(self.fc22(h1))\n",
    "\n",
    "    def reparameterize(self, a, b):\n",
    "        return stickbreakingprocess(20, a, b)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        a, b = self.encode(x)\n",
    "        z = self.reparameterize(a, b)\n",
    "        return self.decode(z), a, b\n",
    "    \n",
    "    def Beta(self, a,b):\n",
    "        return torch.exp(torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a+b))\n",
    "    \n",
    "    def loss_function(self, recon_x, x, a, b, prior_alpha, prior_beta):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "        kl = 1./(1+a*b) * self.Beta(1./a, b)\n",
    "        kl += 1./(2+a*b) * self.Beta(2./a, b)\n",
    "        kl += 1./(3+a*b) * self.Beta(3./a, b)\n",
    "        kl += 1./(4+a*b) * self.Beta(4./a, b)\n",
    "        kl += 1./(5+a*b) * self.Beta(5./a, b)\n",
    "        kl += 1./(6+a*b) * self.Beta(6./a, b)\n",
    "        kl += 1./(7+a*b) * self.Beta(7./a, b)\n",
    "        kl += 1./(8+a*b) * self.Beta(8./a, b)\n",
    "        kl += 1./(9+a*b) * self.Beta(9./a, b)\n",
    "        kl += 1./(10+a*b) * self.Beta(10./a, b)\n",
    "        kl *= (prior_beta-1)*b\n",
    "                                                                                                                                            \n",
    "        kl += (a-prior_alpha)/a * (-np.euler_gamma - torch.digamma(b) - 1/b) #T.psi(self.posterior_b)                                                                                        \n",
    "\n",
    "        # add normalization constants                                                                                                                                                                \n",
    "        kl += torch.log(a*b) #+ torch.log(self.Beta(prior_alpha, prior_beta))\n",
    "\n",
    "        # final term                                                                                                                                                                                 \n",
    "        kl += -(b-1)/b \n",
    "\n",
    "        return BCE + kl.sum()\n",
    "    \n",
    "    def compute_loss_train(self, data, target):\n",
    "        recon_batch, a, b = self(data)\n",
    "        return self.loss_function(recon_batch, data, a, b, torch.Tensor([1]), torch.Tensor([5]))\n",
    "    \n",
    "    def compute_loss_test(self, data, target):\n",
    "        recon_batch, a, b = self(data)\n",
    "        return self.loss_function(recon_batch, data, a, b, 1, 5).item(), recon_batch  # sum up batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 40108.292969\n",
      "Train Epoch: 1 [640/10000 (6%)]\tLoss: 28535.388672\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 18639.339844\n",
      "Train Epoch: 1 [1920/10000 (19%)]\tLoss: 6732.952148\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: -5551.542969\n",
      "Train Epoch: 1 [3200/10000 (32%)]\tLoss: -21250.224609\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: -28925.025391\n",
      "Train Epoch: 1 [4480/10000 (45%)]\tLoss: -45837.699219\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: -65902.265625\n",
      "Train Epoch: 1 [5760/10000 (57%)]\tLoss: -88064.898438\n",
      "Train Epoch: 1 [6400/10000 (64%)]\tLoss: -102332.882812\n",
      "Train Epoch: 1 [7040/10000 (70%)]\tLoss: -141090.687500\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: -173426.468750\n",
      "Train Epoch: 1 [8320/10000 (83%)]\tLoss: -206034.984375\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: -255373.250000\n",
      "Train Epoch: 1 [9600/10000 (96%)]\tLoss: -306094.718750\n",
      "\n",
      "Test set: Average loss: -5136.3508, Reconstruction error: 564488.9375\n",
      "\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: -328507.937500\n",
      "Train Epoch: 2 [640/10000 (6%)]\tLoss: -313714.875000\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: -328747.375000\n",
      "Train Epoch: 2 [1920/10000 (19%)]\tLoss: -337314.656250\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: -324046.218750\n",
      "Train Epoch: 2 [3200/10000 (32%)]\tLoss: -339227.031250\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: -336031.156250\n",
      "Train Epoch: 2 [4480/10000 (45%)]\tLoss: -330590.125000\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: -329764.031250\n",
      "Train Epoch: 2 [5760/10000 (57%)]\tLoss: -332762.187500\n",
      "Train Epoch: 2 [6400/10000 (64%)]\tLoss: -345658.781250\n",
      "Train Epoch: 2 [7040/10000 (70%)]\tLoss: -343454.218750\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: -359884.000000\n",
      "Train Epoch: 2 [8320/10000 (83%)]\tLoss: -342753.781250\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: -352248.656250\n",
      "Train Epoch: 2 [9600/10000 (96%)]\tLoss: -344510.437500\n",
      "\n",
      "Test set: Average loss: -5573.4232, Reconstruction error: 561706.0625\n",
      "\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: -353680.437500\n",
      "Train Epoch: 3 [640/10000 (6%)]\tLoss: -348453.062500\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: -355044.625000\n",
      "Train Epoch: 3 [1920/10000 (19%)]\tLoss: -342812.875000\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: -351051.875000\n",
      "Train Epoch: 3 [3200/10000 (32%)]\tLoss: -353676.031250\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: -365695.562500\n",
      "Train Epoch: 3 [4480/10000 (45%)]\tLoss: -354537.125000\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: -341726.531250\n",
      "Train Epoch: 3 [5760/10000 (57%)]\tLoss: -335578.281250\n",
      "Train Epoch: 3 [6400/10000 (64%)]\tLoss: -369218.937500\n",
      "Train Epoch: 3 [7040/10000 (70%)]\tLoss: -363235.500000\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: -366403.062500\n",
      "Train Epoch: 3 [8320/10000 (83%)]\tLoss: -355021.718750\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: -338205.687500\n",
      "Train Epoch: 3 [9600/10000 (96%)]\tLoss: -362996.250000\n",
      "\n",
      "Test set: Average loss: -5601.3047, Reconstruction error: 563059.375\n",
      "\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: -375832.062500\n",
      "Train Epoch: 4 [640/10000 (6%)]\tLoss: -370493.218750\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: -343813.937500\n",
      "Train Epoch: 4 [1920/10000 (19%)]\tLoss: -361669.750000\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: -350986.125000\n",
      "Train Epoch: 4 [3200/10000 (32%)]\tLoss: -346839.125000\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: -329775.750000\n",
      "Train Epoch: 4 [4480/10000 (45%)]\tLoss: -354458.500000\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: -362153.187500\n",
      "Train Epoch: 4 [5760/10000 (57%)]\tLoss: -341566.250000\n",
      "Train Epoch: 4 [6400/10000 (64%)]\tLoss: -351343.625000\n",
      "Train Epoch: 4 [7040/10000 (70%)]\tLoss: -369404.125000\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: -341030.968750\n",
      "Train Epoch: 4 [8320/10000 (83%)]\tLoss: -384012.156250\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: -353943.031250\n",
      "Train Epoch: 4 [9600/10000 (96%)]\tLoss: -354536.375000\n",
      "\n",
      "Test set: Average loss: -5604.5451, Reconstruction error: 565487.6875\n",
      "\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: -367316.062500\n",
      "Train Epoch: 5 [640/10000 (6%)]\tLoss: -336593.343750\n",
      "Train Epoch: 5 [1280/10000 (13%)]\tLoss: -358164.187500\n",
      "Train Epoch: 5 [1920/10000 (19%)]\tLoss: -345395.187500\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: -356920.062500\n",
      "Train Epoch: 5 [3200/10000 (32%)]\tLoss: -358360.843750\n",
      "Train Epoch: 5 [3840/10000 (38%)]\tLoss: -360765.718750\n",
      "Train Epoch: 5 [4480/10000 (45%)]\tLoss: -363673.906250\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: -353770.250000\n",
      "Train Epoch: 5 [5760/10000 (57%)]\tLoss: -353693.718750\n",
      "Train Epoch: 5 [6400/10000 (64%)]\tLoss: -353940.625000\n",
      "Train Epoch: 5 [7040/10000 (70%)]\tLoss: -362916.000000\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: -359315.031250\n",
      "Train Epoch: 5 [8320/10000 (83%)]\tLoss: -346156.781250\n",
      "Train Epoch: 5 [8960/10000 (89%)]\tLoss: -362892.906250\n",
      "Train Epoch: 5 [9600/10000 (96%)]\tLoss: -371148.031250\n",
      "\n",
      "Test set: Average loss: -5605.2509, Reconstruction error: 566714.8125\n",
      "\n",
      "Train Epoch: 6 [0/10000 (0%)]\tLoss: -352308.968750\n",
      "Train Epoch: 6 [640/10000 (6%)]\tLoss: -342761.468750\n",
      "Train Epoch: 6 [1280/10000 (13%)]\tLoss: -370511.156250\n",
      "Train Epoch: 6 [1920/10000 (19%)]\tLoss: -349736.843750\n",
      "Train Epoch: 6 [2560/10000 (25%)]\tLoss: -345488.250000\n",
      "Train Epoch: 6 [3200/10000 (32%)]\tLoss: -364734.937500\n",
      "Train Epoch: 6 [3840/10000 (38%)]\tLoss: -358027.687500\n",
      "Train Epoch: 6 [4480/10000 (45%)]\tLoss: -363111.625000\n",
      "Train Epoch: 6 [5120/10000 (51%)]\tLoss: -351246.218750\n",
      "Train Epoch: 6 [5760/10000 (57%)]\tLoss: -359884.062500\n",
      "Train Epoch: 6 [6400/10000 (64%)]\tLoss: -340341.875000\n",
      "Train Epoch: 6 [7040/10000 (70%)]\tLoss: -359714.000000\n",
      "Train Epoch: 6 [7680/10000 (76%)]\tLoss: -350042.281250\n",
      "Train Epoch: 6 [8320/10000 (83%)]\tLoss: -357499.968750\n",
      "Train Epoch: 6 [8960/10000 (89%)]\tLoss: -356222.843750\n",
      "Train Epoch: 6 [9600/10000 (96%)]\tLoss: -351588.718750\n",
      "\n",
      "Test set: Average loss: -5604.8795, Reconstruction error: 563039.0\n",
      "\n",
      "Train Epoch: 7 [0/10000 (0%)]\tLoss: -356679.125000\n",
      "Train Epoch: 7 [640/10000 (6%)]\tLoss: -371238.718750\n",
      "Train Epoch: 7 [1280/10000 (13%)]\tLoss: -354299.687500\n",
      "Train Epoch: 7 [1920/10000 (19%)]\tLoss: -361654.750000\n",
      "Train Epoch: 7 [2560/10000 (25%)]\tLoss: -361383.656250\n",
      "Train Epoch: 7 [3200/10000 (32%)]\tLoss: -349743.750000\n",
      "Train Epoch: 7 [3840/10000 (38%)]\tLoss: -354943.625000\n",
      "Train Epoch: 7 [4480/10000 (45%)]\tLoss: -363181.593750\n",
      "Train Epoch: 7 [5120/10000 (51%)]\tLoss: -349990.406250\n",
      "Train Epoch: 7 [5760/10000 (57%)]\tLoss: -372139.437500\n",
      "Train Epoch: 7 [6400/10000 (64%)]\tLoss: -353687.343750\n",
      "Train Epoch: 7 [7040/10000 (70%)]\tLoss: -367828.000000\n",
      "Train Epoch: 7 [7680/10000 (76%)]\tLoss: -362969.906250\n",
      "Train Epoch: 7 [8320/10000 (83%)]\tLoss: -364795.781250\n",
      "Train Epoch: 7 [8960/10000 (89%)]\tLoss: -365344.812500\n",
      "Train Epoch: 7 [9600/10000 (96%)]\tLoss: -364677.031250\n",
      "\n",
      "Test set: Average loss: -5605.2675, Reconstruction error: 563043.1875\n",
      "\n",
      "Train Epoch: 8 [0/10000 (0%)]\tLoss: -355657.125000\n",
      "Train Epoch: 8 [640/10000 (6%)]\tLoss: -351868.218750\n",
      "Train Epoch: 8 [1280/10000 (13%)]\tLoss: -353056.656250\n",
      "Train Epoch: 8 [1920/10000 (19%)]\tLoss: -357902.468750\n",
      "Train Epoch: 8 [2560/10000 (25%)]\tLoss: -357391.218750\n",
      "Train Epoch: 8 [3200/10000 (32%)]\tLoss: -350145.750000\n",
      "Train Epoch: 8 [3840/10000 (38%)]\tLoss: -366408.250000\n",
      "Train Epoch: 8 [4480/10000 (45%)]\tLoss: -361479.125000\n",
      "Train Epoch: 8 [5120/10000 (51%)]\tLoss: -368044.062500\n",
      "Train Epoch: 8 [5760/10000 (57%)]\tLoss: -353394.906250\n",
      "Train Epoch: 8 [6400/10000 (64%)]\tLoss: -361752.531250\n",
      "Train Epoch: 8 [7040/10000 (70%)]\tLoss: -349872.593750\n",
      "Train Epoch: 8 [7680/10000 (76%)]\tLoss: -351507.875000\n",
      "Train Epoch: 8 [8320/10000 (83%)]\tLoss: -343209.906250\n",
      "Train Epoch: 8 [8960/10000 (89%)]\tLoss: -348398.531250\n",
      "Train Epoch: 8 [9600/10000 (96%)]\tLoss: -362590.750000\n",
      "\n",
      "Test set: Average loss: -5604.9718, Reconstruction error: 563211.3125\n",
      "\n",
      "Train Epoch: 9 [0/10000 (0%)]\tLoss: -364976.187500\n",
      "Train Epoch: 9 [640/10000 (6%)]\tLoss: -360174.656250\n",
      "Train Epoch: 9 [1280/10000 (13%)]\tLoss: -356710.687500\n",
      "Train Epoch: 9 [1920/10000 (19%)]\tLoss: -357662.437500\n",
      "Train Epoch: 9 [2560/10000 (25%)]\tLoss: -367324.500000\n",
      "Train Epoch: 9 [3200/10000 (32%)]\tLoss: -353658.093750\n",
      "Train Epoch: 9 [3840/10000 (38%)]\tLoss: -351580.562500\n",
      "Train Epoch: 9 [4480/10000 (45%)]\tLoss: -338268.687500\n",
      "Train Epoch: 9 [5120/10000 (51%)]\tLoss: -367913.937500\n",
      "Train Epoch: 9 [5760/10000 (57%)]\tLoss: -358490.093750\n",
      "Train Epoch: 9 [6400/10000 (64%)]\tLoss: -360199.125000\n",
      "Train Epoch: 9 [7040/10000 (70%)]\tLoss: -353586.093750\n",
      "Train Epoch: 9 [7680/10000 (76%)]\tLoss: -356972.812500\n",
      "Train Epoch: 9 [8320/10000 (83%)]\tLoss: -358755.375000\n",
      "Train Epoch: 9 [8960/10000 (89%)]\tLoss: -348225.500000\n",
      "Train Epoch: 9 [9600/10000 (96%)]\tLoss: -374174.968750\n",
      "\n",
      "Test set: Average loss: -5605.0880, Reconstruction error: 561630.0\n",
      "\n",
      "Train Epoch: 10 [0/10000 (0%)]\tLoss: -358758.468750\n",
      "Train Epoch: 10 [640/10000 (6%)]\tLoss: -373432.406250\n",
      "Train Epoch: 10 [1280/10000 (13%)]\tLoss: -361913.687500\n",
      "Train Epoch: 10 [1920/10000 (19%)]\tLoss: -355590.625000\n",
      "Train Epoch: 10 [2560/10000 (25%)]\tLoss: -354155.156250\n",
      "Train Epoch: 10 [3200/10000 (32%)]\tLoss: -359325.968750\n",
      "Train Epoch: 10 [3840/10000 (38%)]\tLoss: -356139.156250\n",
      "Train Epoch: 10 [4480/10000 (45%)]\tLoss: -369984.937500\n",
      "Train Epoch: 10 [5120/10000 (51%)]\tLoss: -350527.531250\n",
      "Train Epoch: 10 [5760/10000 (57%)]\tLoss: -358739.562500\n",
      "Train Epoch: 10 [6400/10000 (64%)]\tLoss: -344116.687500\n",
      "Train Epoch: 10 [7040/10000 (70%)]\tLoss: -359666.468750\n",
      "Train Epoch: 10 [7680/10000 (76%)]\tLoss: -350685.593750\n",
      "Train Epoch: 10 [8320/10000 (83%)]\tLoss: -352985.093750\n",
      "Train Epoch: 10 [8960/10000 (89%)]\tLoss: -348283.968750\n",
      "Train Epoch: 10 [9600/10000 (96%)]\tLoss: -375103.531250\n",
      "\n",
      "Test set: Average loss: -5605.4607, Reconstruction error: 562269.75\n",
      "\n",
      "Train Epoch: 11 [0/10000 (0%)]\tLoss: -363860.375000\n",
      "Train Epoch: 11 [640/10000 (6%)]\tLoss: -360975.625000\n",
      "Train Epoch: 11 [1280/10000 (13%)]\tLoss: -348938.625000\n",
      "Train Epoch: 11 [1920/10000 (19%)]\tLoss: -355632.656250\n",
      "Train Epoch: 11 [2560/10000 (25%)]\tLoss: -370283.218750\n",
      "Train Epoch: 11 [3200/10000 (32%)]\tLoss: -352751.031250\n",
      "Train Epoch: 11 [3840/10000 (38%)]\tLoss: -348321.437500\n",
      "Train Epoch: 11 [4480/10000 (45%)]\tLoss: -358756.250000\n",
      "Train Epoch: 11 [5120/10000 (51%)]\tLoss: -361291.468750\n",
      "Train Epoch: 11 [5760/10000 (57%)]\tLoss: -357680.625000\n",
      "Train Epoch: 11 [6400/10000 (64%)]\tLoss: -360735.093750\n",
      "Train Epoch: 11 [7040/10000 (70%)]\tLoss: -357688.031250\n",
      "Train Epoch: 11 [7680/10000 (76%)]\tLoss: -347106.406250\n",
      "Train Epoch: 11 [8320/10000 (83%)]\tLoss: -346547.812500\n",
      "Train Epoch: 11 [8960/10000 (89%)]\tLoss: -338799.781250\n",
      "Train Epoch: 11 [9600/10000 (96%)]\tLoss: -353380.312500\n",
      "\n",
      "Test set: Average loss: -5605.1499, Reconstruction error: 565972.4375\n",
      "\n",
      "Train Epoch: 12 [0/10000 (0%)]\tLoss: -357582.625000\n",
      "Train Epoch: 12 [640/10000 (6%)]\tLoss: -362268.218750\n",
      "Train Epoch: 12 [1280/10000 (13%)]\tLoss: -370031.562500\n",
      "Train Epoch: 12 [1920/10000 (19%)]\tLoss: -357765.562500\n",
      "Train Epoch: 12 [2560/10000 (25%)]\tLoss: -352282.031250\n",
      "Train Epoch: 12 [3200/10000 (32%)]\tLoss: -355768.687500\n",
      "Train Epoch: 12 [3840/10000 (38%)]\tLoss: -363262.218750\n",
      "Train Epoch: 12 [4480/10000 (45%)]\tLoss: -346594.000000\n",
      "Train Epoch: 12 [5120/10000 (51%)]\tLoss: -345622.656250\n",
      "Train Epoch: 12 [5760/10000 (57%)]\tLoss: -366077.937500\n",
      "Train Epoch: 12 [6400/10000 (64%)]\tLoss: -367743.218750\n",
      "Train Epoch: 12 [7040/10000 (70%)]\tLoss: -346145.187500\n",
      "Train Epoch: 12 [7680/10000 (76%)]\tLoss: -345112.468750\n",
      "Train Epoch: 12 [8320/10000 (83%)]\tLoss: -346017.906250\n",
      "Train Epoch: 12 [8960/10000 (89%)]\tLoss: -354187.843750\n",
      "Train Epoch: 12 [9600/10000 (96%)]\tLoss: -345068.437500\n",
      "\n",
      "Test set: Average loss: -5604.5430, Reconstruction error: 562526.0625\n",
      "\n",
      "Train Epoch: 13 [0/10000 (0%)]\tLoss: -360948.375000\n",
      "Train Epoch: 13 [640/10000 (6%)]\tLoss: -365000.375000\n",
      "Train Epoch: 13 [1280/10000 (13%)]\tLoss: -359686.062500\n",
      "Train Epoch: 13 [1920/10000 (19%)]\tLoss: -358415.781250\n",
      "Train Epoch: 13 [2560/10000 (25%)]\tLoss: -343970.968750\n",
      "Train Epoch: 13 [3200/10000 (32%)]\tLoss: -364386.218750\n",
      "Train Epoch: 13 [3840/10000 (38%)]\tLoss: -364336.000000\n",
      "Train Epoch: 13 [4480/10000 (45%)]\tLoss: -353016.812500\n",
      "Train Epoch: 13 [5120/10000 (51%)]\tLoss: -355716.187500\n",
      "Train Epoch: 13 [5760/10000 (57%)]\tLoss: -343596.156250\n",
      "Train Epoch: 13 [6400/10000 (64%)]\tLoss: -360628.812500\n",
      "Train Epoch: 13 [7040/10000 (70%)]\tLoss: -349958.062500\n",
      "Train Epoch: 13 [7680/10000 (76%)]\tLoss: -348551.937500\n",
      "Train Epoch: 13 [8320/10000 (83%)]\tLoss: -353070.218750\n",
      "Train Epoch: 13 [8960/10000 (89%)]\tLoss: -350719.343750\n",
      "Train Epoch: 13 [9600/10000 (96%)]\tLoss: -355811.906250\n",
      "\n",
      "Test set: Average loss: -5604.8547, Reconstruction error: 565693.6875\n",
      "\n",
      "Train Epoch: 14 [0/10000 (0%)]\tLoss: -346606.625000\n",
      "Train Epoch: 14 [640/10000 (6%)]\tLoss: -364747.218750\n",
      "Train Epoch: 14 [1280/10000 (13%)]\tLoss: -332832.781250\n",
      "Train Epoch: 14 [1920/10000 (19%)]\tLoss: -365622.937500\n",
      "Train Epoch: 14 [2560/10000 (25%)]\tLoss: -357239.312500\n",
      "Train Epoch: 14 [3200/10000 (32%)]\tLoss: -354348.031250\n",
      "Train Epoch: 14 [3840/10000 (38%)]\tLoss: -362523.031250\n",
      "Train Epoch: 14 [4480/10000 (45%)]\tLoss: -333178.062500\n",
      "Train Epoch: 14 [5120/10000 (51%)]\tLoss: -356048.750000\n",
      "Train Epoch: 14 [5760/10000 (57%)]\tLoss: -358785.906250\n",
      "Train Epoch: 14 [6400/10000 (64%)]\tLoss: -348366.906250\n",
      "Train Epoch: 14 [7040/10000 (70%)]\tLoss: -350324.125000\n",
      "Train Epoch: 14 [7680/10000 (76%)]\tLoss: -339820.343750\n",
      "Train Epoch: 14 [8320/10000 (83%)]\tLoss: -361202.750000\n",
      "Train Epoch: 14 [8960/10000 (89%)]\tLoss: -355348.468750\n",
      "Train Epoch: 14 [9600/10000 (96%)]\tLoss: -366579.781250\n",
      "\n",
      "Test set: Average loss: -5605.0841, Reconstruction error: 560974.0625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SBVAE(k=20).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "for epoch in range(1, 14 + 1): \n",
    "    model.trains(device, train_loader, optimizer, epoch)\n",
    "    model.tests(device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
