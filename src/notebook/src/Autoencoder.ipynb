{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Autoencoder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wittenator/pimai/blob/master/src/notebook/src/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90gWsaxMXxiK",
        "colab_type": "code",
        "outputId": "a8231896-cecb-4544-f351-4ce4997ea027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  !pip install torch torchvision skorch\n",
        "  !pip install hypertools\n",
        "  colab = True\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Collecting skorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/1e/cc4e1f23cd1faab06672f309e0857294aaa80c5f84670f4d3d19b08ab10b/skorch-0.7.0-py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.7.0\n",
            "Collecting hypertools\n",
            "  Downloading https://files.pythonhosted.org/packages/74/85/94f7f6908646fe19fbd36dbcab7e5a5861255f64f4629367dbc52b538a36/hypertools-0.6.2.tar.gz\n",
            "Collecting PPCA>=0.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/16/7f/7195bf3742e19076a21a9c3250a4f11c87153bb4ea3dcaf1077678383b76/ppca-0.0.4-py3-none-any.whl\n",
            "Collecting scikit-learn<0.22,>=0.19.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from hypertools) (0.25.3)\n",
            "Requirement already satisfied: seaborn>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from hypertools) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from hypertools) (3.1.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from hypertools) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from hypertools) (1.17.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hypertools) (0.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from hypertools) (2.21.0)\n",
            "Collecting deepdish\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/39/2a47c852651982bc5eb39212ac110284dd20126bdc7b49bde401a0139f5d/deepdish-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hypertools) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22,>=0.19.1->hypertools) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->hypertools) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->hypertools) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->hypertools) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->hypertools) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->hypertools) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (2.8)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from deepdish->hypertools) (3.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->hypertools) (42.0.2)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->deepdish->hypertools) (2.7.1)\n",
            "Building wheels for collected packages: hypertools\n",
            "  Building wheel for hypertools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hypertools: filename=hypertools-0.6.2-cp36-none-any.whl size=46622 sha256=2c7f32ebeda3f237d46fb147233ca6b474dae26056ab5b7ed75cbb98e49640a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/31/3a/0a3f26ae77857ae19ee5947fd2ee9bf34249fcd9c6c90636c2\n",
            "Successfully built hypertools\n",
            "Installing collected packages: PPCA, scikit-learn, deepdish, hypertools\n",
            "  Found existing installation: scikit-learn 0.22.1\n",
            "    Uninstalling scikit-learn-0.22.1:\n",
            "      Successfully uninstalled scikit-learn-0.22.1\n",
            "Successfully installed PPCA-0.0.4 deepdish-0.3.6 hypertools-0.6.2 scikit-learn-0.21.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TPMgseDXxiZ",
        "colab_type": "code",
        "outputId": "42b9595d-b464-49f8-88f0-19cdd3d9d68d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from datetime import datetime\n",
        "from scipy.signal import sawtooth\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "from torch.distributions import *\n",
        "\n",
        "import skorch\n",
        "import numpy as np\n",
        "import hypertools as hyp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%load_ext tensorboard\n",
        "%matplotlib inline\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f6a0c35c0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaYNKNJqXxip",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koWH_sFdXxit",
        "colab_type": "code",
        "outputId": "3218ccd6-758e-4ff1-c005-3c92f60c4cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': cpu_count(), 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "mnist_train = datasets.MNIST('/data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "mnist_test = datasets.MNIST('/data', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "print(int(len(mnist_train)*prob))\n",
        "prob = 0.5\n",
        "train_occluded = np.array([1] * int(len(mnist_train)*prob) + [0] * int((1-prob)*len(mnist_train)))\n",
        "test_occluded = np.array([1] * int(len(mnist_test)*prob) + [0] * int((1-prob)*len(mnist_test)))\n",
        "np.random.shuffle(train_occluded)\n",
        "np.random.shuffle(test_occluded)\n",
        "\n",
        "\n",
        "mnist_train_occluded =datasets.MNIST('/data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "mnist_train_occluded.targets[train_occluded] = -1\n",
        "mnist_test_occluded = datasets.MNIST('/data', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "mnist_test_occluded.targets[test_occluded] = -1\n",
        "\n",
        "\n",
        "if not colab:\n",
        "  train_loader = DataLoader(Subset(mnist_train, indices=range(1000)), batch_size=64, shuffle=True, **kwargs)\n",
        "  test_loader = DataLoader(Subset(mnist_test, indices=range(1000)), batch_size=1000, shuffle=True, **kwargs)\n",
        "else:\n",
        "  train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True, **kwargs)\n",
        "  test_loader = DataLoader(mnist_test, batch_size=1000, shuffle=True, **kwargs)\n",
        "  \n",
        "  train_loader_occluded = DataLoader(mnist_train_occluded, batch_size=128, shuffle=True, **kwargs)\n",
        "  test_loader_occluded = DataLoader(mnist_test_occluded, batch_size=1000, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbmJnimKXxi3",
        "colab_type": "text"
      },
      "source": [
        "## Generic autoencoder class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vnseljiXxi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        now = datetime.now()\n",
        "        current_time = now.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        self.writer = SummaryWriter(log_dir=\"/data/runs/\"+current_time)\n",
        "        self.embeddings = []\n",
        "        self.embedding_labels =[]\n",
        "    \n",
        "    def trains(self, device, train_loader, optimizer, epoch, epochs):\n",
        "        self.train()\n",
        "        loss_sum = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = self.compute_loss_train(data, target, epoch, epochs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "            self.writer.add_scalar('Loss/train', loss.item(), epoch*len(train_loader)+batch_idx)\n",
        "            \n",
        "    def tests(self, device, test_loader, epoch, epochs):\n",
        "        self.eval()\n",
        "        test_loss = 0\n",
        "        recon = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                loss, output = self.compute_loss_test(data, target, epoch, epochs)\n",
        "                test_loss += loss\n",
        "                recon += F.binary_cross_entropy_with_logits(output, data.view(-1, 784), reduction='none').sum(axis=1).mean()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        recon /= len(test_loader.dataset)\n",
        "\n",
        "        print('\\nTest set: Average loss: {:.4f}, Reconstruction error: {}\\n'.format(\n",
        "            test_loss, recon))\n",
        "        \n",
        "    def add_embedding(self, loader):\n",
        "        with torch.no_grad():\n",
        "            labels = []\n",
        "            embs = []\n",
        "            for data, label in loader:\n",
        "                data, label = data.to(device), label.to(device)\n",
        "                labels.append(label)\n",
        "                recon_batch, a, b = self(data)\n",
        "                emb = self.reparameterize(a,b)\n",
        "                embs.append(emb)\n",
        "            self.embeddings.append(torch.cat(tuple(embs), dim=0).cpu().numpy())\n",
        "            self.embedding_labels = torch.cat(tuple(labels), dim=0).cpu().numpy()\n",
        "            \n",
        "    def visualize_embeddings(self, epoch):\n",
        "        hyp.plot(self.embeddings[epoch], '.', hue=self.embedding_labels, reduce='TSNE', ndims=2, save_path=f'/data/visualizations/{self.__class__.__name__}-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.svg' if not colab else None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca_SLALTXxjC",
        "colab_type": "text"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qKCWvCFXxjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleAutoencoder(Autoencoder):\n",
        "    def __init__(self):\n",
        "        super(SimpleAutoencoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output       \n",
        "    \n",
        "    def compute_loss_train(self, data, target):\n",
        "        output = self(data)\n",
        "        return F.nll_loss(output, target)\n",
        "    \n",
        "    def compute_loss_test(self, data, target):\n",
        "        output = self(data)\n",
        "        return F.nll_loss(output, target, reduction='sum').item(), output  # sum up batch loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTS4EVR9XxjN",
        "colab_type": "code",
        "outputId": "942355ed-9658-4083-8493-34ae556241dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "model = SimpleAutoencoder().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters())\n",
        "\n",
        "# plot model\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "# write to tensorboard\n",
        "#writer.add_image('mnist_images', img_grid)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "for epoch in range(1, 14 + 1):\n",
        "    model.trains(device, train_loader, optimizer, epoch)\n",
        "    model.tests(device, test_loader)\n",
        "    scheduler.step()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2eebc719d3ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: trains() missing 1 required positional argument: 'epochs'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWmNtPJBXxjS",
        "colab_type": "text"
      },
      "source": [
        "## Gaussian Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz2yfQy7XxjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(Autoencoder):\n",
        "    def __init__(self, k=20):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, k)\n",
        "        self.fc22 = nn.Linear(400, k)\n",
        "        self.fc3 = nn.Linear(k, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        #x = self.conv1(x)\n",
        "        #x = F.relu(x)\n",
        "        #x = self.conv2(x)\n",
        "        #x = F.max_pool2d(x, 2)\n",
        "        #x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "    \n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "        # see Appendix B from VAE paper:\n",
        "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "        # https://arxiv.org/abs/1312.6114\n",
        "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        return BCE + KLD\n",
        "    \n",
        "    def compute_loss_train(self, data, target, epoch, epochs):\n",
        "        recon_batch, mu, logvar = self(data)\n",
        "        return self.loss_function(recon_batch, data, mu, logvar)\n",
        "    \n",
        "    def compute_loss_test(self, data, target):\n",
        "        recon_batch, mu, logvar = self(data)\n",
        "        return self.loss_function(recon_batch, data, mu, logvar).item(), recon_batch  # sum up batch loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLJeo93BXxjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = VAE(k=50).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "for epoch in range(1, 100 + 1):\n",
        "    vae.trains(device, train_loader, optimizer, epoch)\n",
        "    vae.tests(device, test_loader)\n",
        "    vae.add_embedding(test_loader)\n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTmgcTeAXxjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae.visualize_embeddings(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bzWadNqn2EdJ",
        "colab": {}
      },
      "source": [
        "sample = torch.randn(64, 20).to(device)\n",
        "sample = vae.decode(sample).reshape(64, 28, 28).cpu().detach().numpy()\n",
        "f, axarr = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "  axarr[i//8,i%8].imshow(sample[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnhhJ8F5Xxjv",
        "colab_type": "text"
      },
      "source": [
        "## Stick-breaking process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P-mNDsMXxjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stickbreakingprocess(a, b):\n",
        "    eps = 10*torch.finfo(torch.float).eps\n",
        "    batch_size = a.size()[0]\n",
        "    \n",
        "    uniform_samples = Uniform(torch.tensor([eps]), torch.tensor([1.0-eps])).rsample(a.size()).squeeze().to(device) if not use_cuda else torch.cuda.FloatTensor(a.size(0), a.size(1)).uniform_().clamp(eps, 1.0-eps)\n",
        "    exp_a = torch.reciprocal(a)\n",
        "    exp_b = torch.reciprocal(b)\n",
        "    km = (1- uniform_samples.pow(exp_b) + eps).pow(exp_a)\n",
        "    \n",
        "    #no Nans are allowed in the matrix\n",
        "    #assert not torch.isnan(km).any().item()\n",
        "    \n",
        "    cumprods = torch.cat((torch.ones([batch_size, 1], device=device), torch.cumprod(1-km, axis=1)), dim=1)\n",
        "    sticks = cumprods[:,:-1]*km\n",
        "    sticks[:, -1] = 1- sticks[:, :-1].sum(axis=1) \n",
        "    return sticks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziPKTj5dXxj4",
        "colab_type": "code",
        "outputId": "be037056-4f97-4076-de96-7e7da0f7c520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "stickbreakingprocess(torch.rand(10,20).to(device), torch.rand(10,20).to(device)).sum(axis=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QheULZUTXxkB",
        "colab_type": "text"
      },
      "source": [
        "## Stick-breaking Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qp8w_wAXxkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SBVAE(Autoencoder):\n",
        "    def __init__(self, k):\n",
        "        super(SBVAE, self).__init__()\n",
        "        self.k = k\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, self.k)\n",
        "        self.fc22 = nn.Linear(400, self.k)\n",
        "        \n",
        "        \n",
        "        self.fc3 = nn.Linear(self.k, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "        self.prior_alpha = torch.Tensor([1]).to(device)\n",
        "        self.prior_beta = torch.Tensor([5]).to(device)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return F.softplus(self.fc21(h1)), F.softplus(self.fc22(h1))\n",
        "\n",
        "    def reparameterize(self, a, b):\n",
        "        return stickbreakingprocess(a, b)\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return self.fc4(h3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a, b = self.encode(x)\n",
        "        z = self.reparameterize(a, b)\n",
        "        return self.decode(z), a, b\n",
        "    \n",
        "    def Beta(self, a,b):\n",
        "        return torch.exp(torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a+b))\n",
        "\n",
        "    def KLD(self, a,b, prior_alpha, prior_beta):\n",
        "        ab = (a*b)\n",
        "        kl = 1/(1+ab) * self.Beta(1/a, b)\n",
        "        kl += 1/(2+ab) * self.Beta(2/a, b)\n",
        "        kl += 1/(3+ab) * self.Beta(3/a, b)\n",
        "        kl += 1/(4+ab) * self.Beta(4/a, b)\n",
        "        kl += 1/(5+ab) * self.Beta(5/a, b)\n",
        "        kl += 1/(6+ab) * self.Beta(6/a, b)\n",
        "        kl += 1/(7+ab) * self.Beta(7/a, b)\n",
        "        kl += 1/(8+ab) * self.Beta(8/a, b)\n",
        "        kl += 1/(9+ab) * self.Beta(9/a, b)\n",
        "        kl += 1/(10+ab) * self.Beta(10/a, b)\n",
        "        kl *= (prior_beta-1)*b\n",
        "                                                                                                                                            \n",
        "        kl += (a-prior_alpha)/a * (-np.euler_gamma - torch.digamma(b) - 1/b) #T.psi(self.posterior_b)                                                                                        \n",
        "\n",
        "        # add normalization constants                                                                                                                                                                \n",
        "        kl += torch.log(ab) + torch.log(self.Beta(prior_alpha, prior_beta))\n",
        "\n",
        "        # final term                                                                                                                                                                                 \n",
        "        kl += -(b-1)/b \n",
        "\n",
        "        return kl\n",
        "    \n",
        "    def loss_function(self, recon_x, x, a, b, prior_alpha, prior_beta, epoch, epochs):\n",
        "        period = 20\n",
        "        BCE = F.binary_cross_entropy_with_logits(recon_x, x.view(-1, 784), reduction='none')\n",
        "        KLD = self.KLD(a,b, prior_alpha, prior_beta)\n",
        "\n",
        "        return len(train_loader)/a.size(0) * torch.mean(1/period*(epoch%period)*KLD.sum(axis=1) + BCE.sum(axis=1))\n",
        "    \n",
        "    def compute_loss_train(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, self.prior_alpha, self.prior_beta, epoch, epochs)\n",
        "    \n",
        "    def compute_loss_test(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, self.prior_alpha, self.prior_beta, epoch, epochs).item(), recon_batch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNemMwJXxkI",
        "colab_type": "code",
        "outputId": "d50caf7b-4152-4d2e-b895-1c853adb5c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sbvae = SBVAE(k=50).to(device)\n",
        "optimizer = optim.Adam(sbvae.parameters(), lr=0.003, betas=(0.95, 0.999))\n",
        "sbvae.writer.add_graph(sbvae, next(iter(train_loader))[0].to(device))\n",
        "\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "epochs = 1000\n",
        "for epoch in range(1, epochs + 1): \n",
        "    sbvae.trains(device, train_loader, optimizer, epoch, epochs)\n",
        "    sbvae.tests(device, test_loader, epoch, epochs)\n",
        "    scheduler.step()\n",
        "    sbvae.add_embedding(test_loader)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1007: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[99, 680] (-0.08027458190917969 vs. 0.0394827164709568) and 100264 other locations (99.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2012.844604\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1359.553589\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 881.275574\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 817.419495\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 814.706360\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 752.392090\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 744.263489\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 729.247192\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 732.111694\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 737.104797\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 728.523254\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 712.473999\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 689.507263\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 726.232727\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 719.469666\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 678.057861\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 694.595825\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 691.063904\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 663.538635\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 663.225037\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 669.738220\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 648.052307\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 630.538391\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 649.191101\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 605.659546\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 632.551514\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 624.349792\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 592.461975\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 582.446960\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 604.642151\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 595.529480\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 599.147522\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 589.968628\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 607.461060\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 563.427246\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 597.855713\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 576.764160\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 561.829163\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 582.057922\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 531.874207\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 535.892639\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 560.250122\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 540.292847\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 515.814514\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 519.321167\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 558.713562\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 532.901306\n",
            "\n",
            "Test set: Average loss: 0.0673, Reconstruction error: 0.1414840817451477\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 521.172974\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 548.778809\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 542.623169\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 526.156067\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 526.450439\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 537.637329\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 533.035583\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 547.677551\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 539.695435\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 531.071106\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 526.497742\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 531.126526\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 528.994019\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 518.140137\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 516.635010\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 517.332886\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 546.150879\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 516.962097\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 535.811035\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 499.339935\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 533.638489\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 532.978088\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 530.488892\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 528.057007\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 513.780334\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 519.255066\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 516.008545\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 485.429291\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 523.257263\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 521.280823\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 511.818909\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 519.791870\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 521.406128\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 516.351135\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 530.299438\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 544.541748\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 522.738281\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 520.213989\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 515.277344\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 517.269836\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 516.230286\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 536.735962\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 515.003784\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 534.178589\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 522.267944\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 506.686615\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 552.088196\n",
            "\n",
            "Test set: Average loss: 0.0658, Reconstruction error: 0.1370658129453659\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 553.492981\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 515.965454\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 515.235535\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 528.052185\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 544.007507\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 529.394592\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 507.353638\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 521.698669\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 523.940796\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 530.876587\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 528.528931\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 510.736115\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 512.316528\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 525.721436\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 536.769714\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 500.548523\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 520.671448\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 527.726685\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 531.269897\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 517.751892\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 527.643188\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 517.122437\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 537.305847\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 520.923157\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 516.658875\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 510.180786\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 513.423218\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 514.875671\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 532.569641\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 509.730225\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 511.704987\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 546.367676\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 519.022827\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 529.209351\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 535.304138\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 552.936584\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 506.182678\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 523.486023\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 507.922516\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 501.882080\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 519.597595\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 529.140686\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 508.848572\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 513.092041\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 522.245789\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 519.017090\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 534.424561\n",
            "\n",
            "Test set: Average loss: 0.0663, Reconstruction error: 0.13702140748500824\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 520.488220\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 544.772400\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 516.536377\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 563.608887\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 533.262939\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 513.631409\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 521.140991\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 526.094971\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 519.990417\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 516.088196\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 513.311584\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 518.421570\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 549.317017\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 518.300354\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 536.758545\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 501.518341\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 524.969971\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 534.057861\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 513.329163\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 504.061981\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 558.897095\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 508.558411\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 504.380310\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 540.994446\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 528.761902\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 510.477539\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 509.639984\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 515.587341\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 501.170135\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 536.152954\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 524.974121\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 522.721191\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 533.232910\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 544.082092\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 521.795776\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 554.085510\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 517.828918\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 519.899536\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 508.069977\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 525.433594\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 535.257690\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 506.024170\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 514.370850\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 505.847443\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 534.428162\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 523.270142\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 535.098511\n",
            "\n",
            "Test set: Average loss: 0.0668, Reconstruction error: 0.13698545098304749\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 540.179749\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 524.098206\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 525.812866\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 523.924927\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 531.886230\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 517.445068\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 536.214417\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 524.464661\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 527.980347\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 527.323730\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 527.533447\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 553.513977\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 534.889587\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 538.204590\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 527.741577\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 527.244019\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 565.213440\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 542.665710\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 530.950317\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 537.689575\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 527.786743\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 534.613953\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 524.407776\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 546.297180\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 527.750000\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 544.172791\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 529.647644\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 540.456177\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 541.668091\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 546.394287\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 525.649475\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 522.846863\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 521.102722\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 525.304565\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 527.696899\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 527.997009\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 529.225830\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 536.173035\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 538.427979\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 507.055786\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 520.037354\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 523.009583\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 519.267456\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 517.701477\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 539.867737\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 528.182190\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 509.821106\n",
            "\n",
            "Test set: Average loss: 0.0675, Reconstruction error: 0.1371227204799652\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 525.261658\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 524.802734\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 531.941284\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 527.148987\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 533.831970\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 527.211243\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 548.724426\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 560.375488\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 549.560791\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 543.451294\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 530.176941\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 538.803528\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 518.234619\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 506.636353\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 537.683594\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 523.040771\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 531.773132\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 498.770660\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 537.056458\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 513.761292\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 515.127991\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 515.146301\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 538.687439\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 526.427063\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 530.857727\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 538.457031\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 518.822205\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 536.000549\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 528.146057\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 531.643188\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 532.905151\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 529.732422\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 537.374329\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 556.401001\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 500.101135\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 508.398682\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 527.090271\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 542.662903\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 534.260254\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 539.072632\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 542.174255\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 542.236206\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 539.262268\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 529.709534\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 533.004028\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 505.188599\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 511.635101\n",
            "\n",
            "Test set: Average loss: 0.0682, Reconstruction error: 0.13716447353363037\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 539.460205\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 537.647827\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 534.046814\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 530.542053\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 543.273010\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 551.641907\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 563.003418\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 516.359436\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 541.658813\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 554.543152\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 560.308777\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 533.665161\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 536.659912\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 523.391541\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 564.754944\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 548.012695\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 555.890259\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 525.948181\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 568.429871\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 540.994690\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 550.114990\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 523.054016\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 545.478821\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 539.897949\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 544.005615\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 550.469421\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 536.330505\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 558.640137\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 534.988770\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 551.923706\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 524.496521\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 562.731689\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 560.218994\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 524.328735\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 560.346069\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 537.702637\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 519.176514\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 532.510193\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 533.149841\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 533.086975\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 512.202942\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 536.746399\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 532.954224\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 535.578735\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 514.767212\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 531.980042\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 540.894165\n",
            "\n",
            "Test set: Average loss: 0.0688, Reconstruction error: 0.13707146048545837\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 570.962341\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 553.477844\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 540.211914\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 543.090576\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 540.206787\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 542.395325\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 536.872681\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 542.006287\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 512.540222\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 528.637817\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 525.700500\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 514.419495\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 544.321716\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 563.234375\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 537.437866\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 543.087158\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 539.989868\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 565.595398\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 528.834778\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 553.664673\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 522.273254\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 545.249329\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 539.401062\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 557.193542\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 554.648621\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 534.819397\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 539.520142\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 544.257874\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 539.049194\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 536.587769\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 558.059753\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 547.414673\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 537.623840\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 547.512817\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 523.660034\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 525.858276\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 560.042236\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 545.440552\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 564.241028\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 545.957214\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 553.778564\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 568.291687\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 544.903503\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 547.402100\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 524.350281\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 571.549377\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 551.053833\n",
            "\n",
            "Test set: Average loss: 0.0695, Reconstruction error: 0.13718029856681824\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 564.324341\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 534.666870\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 556.293335\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 574.033630\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 525.924377\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 543.018066\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 560.220459\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 547.357788\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 568.084351\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 551.013123\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 574.943359\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 519.871643\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 550.313965\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 562.170349\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 543.358276\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 539.564270\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 545.245972\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 539.500916\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 573.280518\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 524.781799\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 547.872070\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 570.523438\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 543.797180\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 559.941406\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 541.470947\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 529.795532\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 584.415955\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 555.188416\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 547.774780\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 561.116028\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 554.837036\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 541.686157\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 545.637817\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 542.078552\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 536.782227\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 559.027771\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 539.558472\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 552.363403\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 578.214966\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 577.386414\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 527.012817\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 569.939514\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 516.801147\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 557.558533\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 569.804749\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 555.331543\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 535.114441\n",
            "\n",
            "Test set: Average loss: 0.0701, Reconstruction error: 0.13709492981433868\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 539.009216\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 556.757263\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 567.029968\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 543.629578\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 552.713989\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 544.078186\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 563.245056\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 551.784119\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 557.421143\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 566.901794\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 556.479919\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 565.349243\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 551.593811\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 557.911011\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 563.916321\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 566.331116\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 582.147278\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 568.097107\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 555.217957\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 540.539429\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 583.464478\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 534.828125\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 559.024475\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 544.887939\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 541.343567\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 551.544189\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 547.828857\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 561.636414\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 571.370239\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 547.838196\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 579.868713\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 544.420471\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 566.851562\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 553.386292\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 560.172546\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 545.598389\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 554.829041\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 546.676941\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 552.322754\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 573.060791\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 543.897644\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 536.488281\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 562.246887\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 548.486755\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 558.814453\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 575.291443\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 573.366211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-aae4c358b675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-850d1fd2912e>\u001b[0m in \u001b[0;36mtrains\u001b[0;34m(self, device, train_loader, optimizer, epoch, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5aDfJvHXxkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sbvae.visualize_embeddings(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJstAFVO7Lls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.distributions.beta import Beta\n",
        "sample = Beta(torch.tensor([1.0]), torch.tensor([5.0])).rsample([64,50]).squeeze().to(device)\n",
        "cumprods = torch.cat((torch.ones([64, 1], device=device), torch.cumprod(1-sample, axis=1)), dim=1)\n",
        "sample = cumprods[:,:-1]*sample\n",
        "sample[:, -1] = 1- sample[:, :-1].sum(axis=1)\n",
        "sample = torch.sigmoid(sbvae.decode(sample)).reshape(64, 28, 28).cpu().detach().numpy()\n",
        "f, axarr = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "  axarr[i//8,i%8].imshow(sample[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA5Txpn2Djld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSSBVAE(SBVAE):\n",
        "    def __init__(self, k):\n",
        "        super(SSSBVAE, self).__init__(k)\n",
        "  \n",
        "        self.fc23 = nn.Linear(400, 10)\n",
        "        self.fc3 = nn.Linear(self.k + 10, 400)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return F.softplus(self.fc21(h1)), F.softplus(self.fc22(h1)), F.softplus(self.fc23(h1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        a, b, y = self.encode(x)\n",
        "        z = self.reparameterize(a, b)\n",
        "        z = torch.cat((z, y/torch.norm(y, p=1)),dim=1)\n",
        "        return self.decode(z), a, b, y/torch.norm(y, p=1)\n",
        "    \n",
        "    def loss_function(self, recon_x, x, a, b, y, y_true, prior_alpha, prior_beta, epoch, epochs):\n",
        "        period = 20\n",
        "        BCE = F.binary_cross_entropy_with_logits(recon_x, x.view(-1, 784), reduction='none')\n",
        "        KLD = self.KLD(a,b, prior_alpha, prior_beta)\n",
        "\n",
        "        log_y = F.binary_cross_entropy(y, torch.eye(10, device=device)[y_true], reduction='none').sum(axis=1)\n",
        "        ent_y = Categorical(probs=y).entropy()\n",
        "\n",
        "        y_recon = torch.where(y_true !=-1, log_y, ent_y)\n",
        "\n",
        "        eye_batch = torch.eye(10, device=device).unsqueeze(0).expand(a.size(0), -1, -1)\n",
        "        y_batch = y.unsqueeze(2).expand(-1, -1, 10)\n",
        "\n",
        "        factor = torch.where(y_true !=-1, torch.ones(a.size(0), device=device), F.binary_cross_entropy(y_batch, eye_batch, reduction='none').sum(axis=(1,2))) \n",
        "\n",
        "        term = 1/period*(epoch%period)*KLD.sum(axis=1) + BCE.sum(axis=1)*factor + y_recon\n",
        "\n",
        "        return len(train_loader)/a.size(0) * (torch.mean(term))\n",
        "\n",
        "    def compute_loss_train(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b, y = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, y, target, self.prior_alpha, self.prior_beta, epoch, epochs)\n",
        "    \n",
        "    def compute_loss_test(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b, y = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, y, target, self.prior_alpha, self.prior_beta, epoch, epochs).item(), recon_batch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdJtNpRCiIUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af4f93a9-3882-4a7c-ce60-f996cfdaa4bf"
      },
      "source": [
        "sssbvae = SSSBVAE(k=50).to(device)\n",
        "optimizer = optim.Adam(sssbvae.parameters(), lr=0.003, betas=(0.95, 0.999))\n",
        "sssbvae.writer.add_graph(sssbvae, next(iter(train_loader_occluded))[0].to(device))\n",
        "\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "epochs = 1000\n",
        "for epoch in range(1, epochs + 1): \n",
        "    sssbvae.trains(device, train_loader_occluded, optimizer, epoch, epochs)\n",
        "    sssbvae.tests(device, test_loader_occluded, epoch, epochs)\n",
        "    scheduler.step()\n",
        "    #sssbvae.add_embedding(test_loader)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1007: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[5, 417] (-0.1457788348197937 vs. 0.005965184420347214) and 100190 other locations (99.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2044.861328\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1345.036499\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 903.317810\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 820.089355\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 813.921570\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 772.905151\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 766.606323\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 757.802734\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 767.392883\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 729.460571\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 717.513428\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 696.684998\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 679.220337\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 696.734009\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 691.558411\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 665.027466\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 631.781433\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 644.805908\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 653.422363\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 651.993103\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 650.021240\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 653.765686\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 628.210815\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 621.258057\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 621.010315\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 611.885620\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 578.451477\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 580.437317\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 604.693176\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 592.047180\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 561.704529\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 564.499390\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 568.937866\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 566.187134\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 540.806274\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 554.446899\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 564.603455\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 550.209412\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 555.616943\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 544.488831\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 528.274414\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 528.832825\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 541.760315\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 537.882751\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 538.843506\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 528.297363\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 525.708069\n",
            "\n",
            "Test set: Average loss: 0.0694, Reconstruction error: 0.13428214192390442\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 532.864929\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 493.028809\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 515.281494\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 523.155151\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 508.472870\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 521.670227\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 512.351990\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 510.368591\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 514.170471\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 548.568909\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 517.588196\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 519.513977\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 518.113037\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 514.736023\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 521.144348\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 521.462463\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 531.719604\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 521.797913\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 512.938965\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 522.419739\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 529.489136\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 498.658478\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 520.685120\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 527.683289\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 520.137207\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 498.764801\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 500.380981\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 496.114868\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 513.460205\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 514.065125\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 507.998413\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 520.105103\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 521.003113\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 485.091553\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 508.109528\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 509.423096\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 501.511414\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 528.211304\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 522.220703\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 504.282471\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 504.368256\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 503.578918\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 505.556427\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 523.953430\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 502.492371\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 475.915680\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 525.293396\n",
            "\n",
            "Test set: Average loss: 0.0675, Reconstruction error: 0.12974505126476288\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 505.189453\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 534.601624\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 502.006317\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 526.746033\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 531.868591\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 519.881165\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 524.713562\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 529.253906\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 506.546967\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 522.458679\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 523.286804\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 504.268158\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 517.339172\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 520.055420\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 527.344421\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 502.308777\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 518.509460\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 525.572998\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 524.893188\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 526.571838\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 507.386505\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 530.701599\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 504.904083\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 543.431702\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 500.350616\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 505.124634\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 504.201752\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 493.214142\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 507.504974\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 512.681580\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 497.741333\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 502.670105\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 520.499695\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 489.601196\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 512.924927\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 489.905212\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 544.969360\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 529.894470\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 504.919495\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 496.707123\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 500.272552\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 517.597229\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 494.479584\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 530.646545\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 509.118500\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 492.124878\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 514.708740\n",
            "\n",
            "Test set: Average loss: 0.0680, Reconstruction error: 0.12948179244995117\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 548.804749\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 512.221008\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 509.851471\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 512.438599\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 500.134979\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 520.248596\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 531.031860\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 517.890076\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 504.256653\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 507.194794\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 502.212158\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 522.604370\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 524.912598\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 479.982697\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 502.897278\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 506.616577\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 535.738647\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 526.210693\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 529.738159\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 511.370361\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 520.988892\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 524.983582\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 512.967957\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 515.004761\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 496.297241\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 510.821625\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 529.276306\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 528.820862\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 517.170532\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 523.247864\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 525.791199\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 516.932251\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 524.508606\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 491.153107\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 547.423340\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 532.782532\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 507.233154\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 524.608398\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 525.092041\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 522.592957\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 518.353149\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 522.577515\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 508.786865\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 525.103027\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 516.253662\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 504.033508\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 519.574829\n",
            "\n",
            "Test set: Average loss: 0.0687, Reconstruction error: 0.12965109944343567\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 520.818481\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 510.654266\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 500.242096\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 510.445129\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 517.180115\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 515.113831\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 515.415466\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 512.093750\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 506.769989\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 532.729187\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 524.957153\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 521.153503\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 536.006226\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 515.932434\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 528.302429\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 506.567932\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 527.549194\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 518.521912\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 521.334839\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 543.887573\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 526.522217\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 516.435303\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 512.860962\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 529.426941\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 536.999634\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 510.051636\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 513.982056\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 532.060486\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 526.524475\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 522.831909\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 534.547302\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 526.308655\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 531.375977\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 528.609924\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 514.488892\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 527.426208\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 526.266724\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 520.929626\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 526.614685\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 513.528259\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 533.019348\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 522.119202\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 517.225891\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 539.548767\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 512.395935\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 548.685974\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 510.486816\n",
            "\n",
            "Test set: Average loss: 0.0694, Reconstruction error: 0.12972687184810638\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 519.540527\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 506.829254\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 518.484924\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 531.058655\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 528.744507\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 536.222717\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 544.567688\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 527.542908\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 531.155457\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 541.425049\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 533.176453\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 523.328064\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 531.146851\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 550.301392\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 517.065491\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 553.570679\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 516.044495\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 536.278442\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 549.633728\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 533.768860\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 528.834412\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 533.071228\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 513.048035\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 566.745667\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 538.836914\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 539.786621\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 547.300781\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 529.692871\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 544.154968\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 542.618652\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 518.817993\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 538.514221\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 517.985229\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 507.948395\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 538.491028\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 526.850342\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 517.521606\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 518.437012\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 537.217407\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 515.783752\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 528.890259\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 536.655884\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 534.771729\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 516.399963\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 522.074890\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 538.837830\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 505.647766\n",
            "\n",
            "Test set: Average loss: 0.0699, Reconstruction error: 0.12969465553760529\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 518.322449\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 531.535950\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 531.619812\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 535.683350\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 532.254272\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 561.573853\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 523.763916\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 535.712830\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 540.700195\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 516.842224\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 540.122131\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 513.394165\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 524.922546\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 544.332764\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 536.137146\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 518.746155\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 528.723083\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 533.031921\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 546.022156\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 550.631653\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 541.376892\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 531.389709\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 532.671082\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 535.079529\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 558.014893\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 553.367188\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 529.861267\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 525.494446\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 524.170959\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 545.531860\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 539.955200\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 519.198853\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 814.019409\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 538.787964\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 542.514160\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 515.335938\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 519.393860\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 541.737183\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 530.892334\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 547.133911\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 534.178101\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 520.675171\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 538.590637\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 530.252625\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 528.882446\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 547.882874\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 533.441833\n",
            "\n",
            "Test set: Average loss: 0.0707, Reconstruction error: 0.12986046075820923\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 525.356750\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 552.215637\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 542.719360\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 549.905457\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 521.041992\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 543.466125\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 530.435791\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 540.727295\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 543.966980\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 546.938599\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 542.602966\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 555.039978\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 540.449219\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 534.520630\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 557.594727\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 532.508301\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 526.411560\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 556.298828\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 547.196228\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 532.623169\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 538.101685\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 540.449280\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 541.981628\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 538.581604\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 540.042175\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 510.353546\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 548.795532\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 555.644653\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 521.647888\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 556.077576\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 544.368591\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 534.866455\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 549.027344\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 540.320740\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 541.974060\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 536.685669\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 554.990662\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 532.783264\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 554.641785\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 561.979492\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 560.679810\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 539.970947\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 532.960999\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 540.223816\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 561.741821\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 535.000122\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 514.899231\n",
            "\n",
            "Test set: Average loss: 0.0713, Reconstruction error: 0.12972044944763184\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 542.996216\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 506.542603\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 525.416382\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 543.880005\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 541.980713\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 570.502625\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 528.450378\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 550.594543\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 530.614258\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 561.029053\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 572.112244\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 549.856018\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 527.516296\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 550.351990\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 550.288208\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 550.830994\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 529.243103\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 533.114136\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 526.440369\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 545.054321\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 558.531555\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 532.418518\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 559.642822\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 567.164856\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 551.942566\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 537.656433\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 529.348816\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 568.268372\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 545.420654\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 544.882568\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 535.706970\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 549.240845\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 558.542175\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 543.804443\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 556.521240\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 539.581299\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 531.494568\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 538.477295\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 537.607788\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 565.192688\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 549.430664\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 550.081604\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 547.261230\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 556.663757\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 545.484436\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 555.518555\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 557.797424\n",
            "\n",
            "Test set: Average loss: 0.0719, Reconstruction error: 0.12963242828845978\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 564.647461\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 552.693665\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 549.740784\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 565.264587\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 557.021362\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 563.173279\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 558.286255\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 559.665100\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 570.119629\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 568.745178\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 576.277588\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 587.324036\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 532.042847\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 549.983459\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 560.299011\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 536.424438\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 543.894348\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 572.850952\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 523.706055\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 531.231567\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 550.935120\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 539.606750\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 561.835571\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 543.419128\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 560.265381\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 537.016785\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 559.296326\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 563.954224\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 531.270447\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 806.469849\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 529.812927\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 553.104187\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 545.858459\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 537.838135\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 541.457153\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 563.216675\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 554.004456\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 539.964355\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 536.247559\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 534.565125\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 556.066895\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 534.818787\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 533.889160\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 558.478821\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 541.882080\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 541.725708\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 522.426758\n",
            "\n",
            "Test set: Average loss: 0.0727, Reconstruction error: 0.1297726333141327\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 531.870605\n",
            "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 541.333008\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 561.333191\n",
            "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 543.868652\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 556.943237\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 542.983459\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 548.373108\n",
            "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 558.583374\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 545.722900\n",
            "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 537.057312\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 549.597534\n",
            "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 547.045532\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 564.984802\n",
            "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 539.623901\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 552.208801\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 545.809021\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 558.373413\n",
            "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 543.864746\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 555.477600\n",
            "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 553.259644\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 547.763123\n",
            "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 537.386719\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 586.709656\n",
            "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 552.832581\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 551.234070\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 540.310669\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 539.410889\n",
            "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 556.293518\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 538.495300\n",
            "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 553.295898\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 962.291077\n",
            "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 552.932800\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 551.438965\n",
            "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 569.755554\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 554.039490\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 543.192017\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 561.527161\n",
            "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 564.971313\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 524.339233\n",
            "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 566.581177\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 541.782898\n",
            "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 558.244324\n",
            "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 569.680420\n",
            "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 536.216553\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 528.303528\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 533.393372\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 579.972778\n",
            "\n",
            "Test set: Average loss: 0.0733, Reconstruction error: 0.12964648008346558\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 529.096191\n",
            "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 561.312866\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 553.708069\n",
            "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 581.664368\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 574.321838\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 560.163879\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 547.781799\n",
            "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 540.848328\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 547.409546\n",
            "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 557.215210\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 557.764954\n",
            "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 563.867188\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 557.526123\n",
            "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 561.537231\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 564.013489\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 554.116760\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 544.736633\n",
            "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 561.943298\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 569.125793\n",
            "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 566.872803\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 556.286926\n",
            "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 576.935974\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 565.524597\n",
            "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 555.525513\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 557.210999\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 572.733826\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 557.004028\n",
            "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 552.243286\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 550.244629\n",
            "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 554.881836\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 545.773804\n",
            "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 567.259888\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 558.123169\n",
            "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 551.346497\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 560.311890\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 555.933167\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 569.189453\n",
            "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 561.742126\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 549.274841\n",
            "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 553.156616\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 557.427795\n",
            "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 560.252869\n",
            "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 565.344543\n",
            "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 559.581116\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 562.914307\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 544.232422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-22d0cdcc9bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msssbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_occluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msssbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_occluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-850d1fd2912e>\u001b[0m in \u001b[0;36mtrains\u001b[0;34m(self, device, train_loader, optimizer, epoch, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-ce1494e6d8d5>\u001b[0m in \u001b[0;36mcompute_loss_train\u001b[0;34m(self, data, target, epoch, epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-ce1494e6d8d5>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(self, recon_x, x, a, b, y, y_true, prior_alpha, prior_beta, epoch, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mperiod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mBCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mKLD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_beta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mlog_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-7672db404ab0>\u001b[0m in \u001b[0;36mKLD\u001b[0;34m(self, a, b, prior_alpha, prior_beta)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mkl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mkl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mkl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-7672db404ab0>\u001b[0m in \u001b[0;36mBeta\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mBeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_beta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Hpkmer3wEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "862b556c-b9fb-4075-f568-dda95f12d74e"
      },
      "source": [
        "sample = Beta(torch.tensor([1.0]), torch.tensor([5.0])).rsample([64,50]).squeeze().to(device)\n",
        "nums = np.random.randint(10, size=64)\n",
        "onehot = torch.eye(10, device=device)[nums]\n",
        "\n",
        "cumprods = torch.cat((torch.ones([64, 1], device=device), torch.cumprod(1-sample, axis=1)), dim=1)\n",
        "sample = cumprods[:,:-1]*sample\n",
        "sample[:, -1] = 1- sample[:, :-1].sum(axis=1)\n",
        "print(nums)\n",
        "inputs = torch.cat((sample, onehot), dim=1)\n",
        "sample = torch.sigmoid(sssbvae.decode(inputs)).reshape(64, 28, 28).cpu().detach().numpy()\n",
        "f, axarr = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "  axarr[i//8,i%8].imshow(sample[i])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7 2 4 4 6 7 8 9 6 8 6 6 7 6 0 4 2 1 9 2 6 0 2 4 5 3 8 7 4 2 6 9 2 5 4 2 8\n",
            " 9 1 1 7 2 9 3 6 7 4 1 1 9 3 7 6 2 9 3 6 6 1 4 5 8 6 8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAD8CAYAAABaZT40AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncNAJJZRQQoep/j+Dg6G9YjjcCAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 64 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfmG5q9g0ggp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /data/runs/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}