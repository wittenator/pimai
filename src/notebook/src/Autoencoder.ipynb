{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Autoencoder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wittenator/pimai/blob/master/src/notebook/src/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90gWsaxMXxiK",
        "colab_type": "code",
        "outputId": "a8231896-cecb-4544-f351-4ce4997ea027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  !pip install torch torchvision skorch\n",
        "  !pip install hypertools\n",
        "  colab = True\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Collecting skorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/1e/cc4e1f23cd1faab06672f309e0857294aaa80c5f84670f4d3d19b08ab10b/skorch-0.7.0-py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.7.0\n",
            "Collecting hypertools\n",
            "  Downloading https://files.pythonhosted.org/packages/74/85/94f7f6908646fe19fbd36dbcab7e5a5861255f64f4629367dbc52b538a36/hypertools-0.6.2.tar.gz\n",
            "Collecting PPCA>=0.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/16/7f/7195bf3742e19076a21a9c3250a4f11c87153bb4ea3dcaf1077678383b76/ppca-0.0.4-py3-none-any.whl\n",
            "Collecting scikit-learn<0.22,>=0.19.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from hypertools) (0.25.3)\n",
            "Requirement already satisfied: seaborn>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from hypertools) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from hypertools) (3.1.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from hypertools) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from hypertools) (1.17.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hypertools) (0.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from hypertools) (2.21.0)\n",
            "Collecting deepdish\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/39/2a47c852651982bc5eb39212ac110284dd20126bdc7b49bde401a0139f5d/deepdish-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hypertools) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22,>=0.19.1->hypertools) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->hypertools) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->hypertools) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->hypertools) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->hypertools) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->hypertools) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (2.8)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from deepdish->hypertools) (3.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->hypertools) (42.0.2)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->deepdish->hypertools) (2.7.1)\n",
            "Building wheels for collected packages: hypertools\n",
            "  Building wheel for hypertools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hypertools: filename=hypertools-0.6.2-cp36-none-any.whl size=46622 sha256=2c7f32ebeda3f237d46fb147233ca6b474dae26056ab5b7ed75cbb98e49640a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/31/3a/0a3f26ae77857ae19ee5947fd2ee9bf34249fcd9c6c90636c2\n",
            "Successfully built hypertools\n",
            "Installing collected packages: PPCA, scikit-learn, deepdish, hypertools\n",
            "  Found existing installation: scikit-learn 0.22.1\n",
            "    Uninstalling scikit-learn-0.22.1:\n",
            "      Successfully uninstalled scikit-learn-0.22.1\n",
            "Successfully installed PPCA-0.0.4 deepdish-0.3.6 hypertools-0.6.2 scikit-learn-0.21.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TPMgseDXxiZ",
        "colab_type": "code",
        "outputId": "42b9595d-b464-49f8-88f0-19cdd3d9d68d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from datetime import datetime\n",
        "from scipy.signal import sawtooth\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "from torch.distributions import *\n",
        "\n",
        "import skorch\n",
        "import numpy as np\n",
        "import hypertools as hyp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%load_ext tensorboard\n",
        "%matplotlib inline\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f6a0c35c0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaYNKNJqXxip",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koWH_sFdXxit",
        "colab_type": "code",
        "outputId": "eeeb1337-a875-4db3-9d53-8cd05679cc1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': cpu_count(), 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "mnist_train = datasets.MNIST('/data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "mnist_test = datasets.MNIST('/data', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "print(int(len(mnist_train)*prob))\n",
        "prob = 0.05\n",
        "train_occluded = np.array([1] * int(len(mnist_train)*prob) + [0] * int((1-prob)*len(mnist_train)))\n",
        "test_occluded = np.array([1] * int(len(mnist_test)*prob) + [0] * int((1-prob)*len(mnist_test)))\n",
        "np.random.shuffle(train_occluded)\n",
        "np.random.shuffle(test_occluded)\n",
        "\n",
        "\n",
        "mnist_train_occluded =datasets.MNIST('/data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "mnist_train_occluded.targets[train_occluded] = -1\n",
        "mnist_test_occluded = datasets.MNIST('/data', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "mnist_test_occluded.targets[test_occluded] = -1\n",
        "\n",
        "\n",
        "if not colab:\n",
        "  train_loader = DataLoader(Subset(mnist_train, indices=range(1000)), batch_size=64, shuffle=True, **kwargs)\n",
        "  test_loader = DataLoader(Subset(mnist_test, indices=range(1000)), batch_size=1000, shuffle=True, **kwargs)\n",
        "else:\n",
        "  train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True, **kwargs)\n",
        "  test_loader = DataLoader(mnist_test, batch_size=1000, shuffle=True, **kwargs)\n",
        "  \n",
        "  train_loader_occluded = DataLoader(mnist_train_occluded, batch_size=128, shuffle=True, **kwargs)\n",
        "  test_loader_occluded = DataLoader(mnist_test_occluded, batch_size=1000, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbmJnimKXxi3",
        "colab_type": "text"
      },
      "source": [
        "## Generic autoencoder class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vnseljiXxi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        now = datetime.now()\n",
        "        current_time = now.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        self.writer = SummaryWriter(log_dir=\"/data/runs/\"+current_time)\n",
        "        self.embeddings = []\n",
        "        self.embedding_labels =[]\n",
        "    \n",
        "    def trains(self, device, train_loader, optimizer, epoch, epochs):\n",
        "        self.train()\n",
        "        loss_sum = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = self.compute_loss_train(data, target, epoch, epochs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "            self.writer.add_scalar('Loss/train', loss.item(), epoch*len(train_loader)+batch_idx)\n",
        "            \n",
        "    def tests(self, device, test_loader, epoch, epochs):\n",
        "        self.eval()\n",
        "        test_loss = 0\n",
        "        recon = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                loss, output = self.compute_loss_test(data, target, epoch, epochs)\n",
        "                test_loss += loss\n",
        "                recon += F.binary_cross_entropy_with_logits(output, data.view(-1, 784), reduction='none').sum(axis=1).mean()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        recon /= len(test_loader.dataset)\n",
        "\n",
        "        print('\\nTest set: Average loss: {:.4f}, Reconstruction error: {}\\n'.format(\n",
        "            test_loss, recon))\n",
        "        \n",
        "    def add_embedding(self, loader):\n",
        "        with torch.no_grad():\n",
        "            labels = []\n",
        "            embs = []\n",
        "            for data, label in loader:\n",
        "                data, label = data.to(device), label.to(device)\n",
        "                labels.append(label)\n",
        "                recon_batch, a, b = self(data)\n",
        "                emb = self.reparameterize(a,b)\n",
        "                embs.append(emb)\n",
        "            self.embeddings.append(torch.cat(tuple(embs), dim=0).cpu().numpy())\n",
        "            self.embedding_labels = torch.cat(tuple(labels), dim=0).cpu().numpy()\n",
        "            \n",
        "    def visualize_embeddings(self, epoch):\n",
        "        hyp.plot(self.embeddings[epoch], '.', hue=self.embedding_labels, reduce='TSNE', ndims=2, save_path=f'/data/visualizations/{self.__class__.__name__}-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.svg' if not colab else None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca_SLALTXxjC",
        "colab_type": "text"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qKCWvCFXxjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleAutoencoder(Autoencoder):\n",
        "    def __init__(self):\n",
        "        super(SimpleAutoencoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output       \n",
        "    \n",
        "    def compute_loss_train(self, data, target):\n",
        "        output = self(data)\n",
        "        return F.nll_loss(output, target)\n",
        "    \n",
        "    def compute_loss_test(self, data, target):\n",
        "        output = self(data)\n",
        "        return F.nll_loss(output, target, reduction='sum').item(), output  # sum up batch loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTS4EVR9XxjN",
        "colab_type": "code",
        "outputId": "942355ed-9658-4083-8493-34ae556241dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "model = SimpleAutoencoder().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters())\n",
        "\n",
        "# plot model\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "# write to tensorboard\n",
        "#writer.add_image('mnist_images', img_grid)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "for epoch in range(1, 14 + 1):\n",
        "    model.trains(device, train_loader, optimizer, epoch)\n",
        "    model.tests(device, test_loader)\n",
        "    scheduler.step()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2eebc719d3ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: trains() missing 1 required positional argument: 'epochs'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWmNtPJBXxjS",
        "colab_type": "text"
      },
      "source": [
        "## Gaussian Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz2yfQy7XxjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(Autoencoder):\n",
        "    def __init__(self, k=20):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, k)\n",
        "        self.fc22 = nn.Linear(400, k)\n",
        "        self.fc3 = nn.Linear(k, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        #x = self.conv1(x)\n",
        "        #x = F.relu(x)\n",
        "        #x = self.conv2(x)\n",
        "        #x = F.max_pool2d(x, 2)\n",
        "        #x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "    \n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "        # see Appendix B from VAE paper:\n",
        "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "        # https://arxiv.org/abs/1312.6114\n",
        "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        return BCE + KLD\n",
        "    \n",
        "    def compute_loss_train(self, data, target, epoch, epochs):\n",
        "        recon_batch, mu, logvar = self(data)\n",
        "        return self.loss_function(recon_batch, data, mu, logvar)\n",
        "    \n",
        "    def compute_loss_test(self, data, target):\n",
        "        recon_batch, mu, logvar = self(data)\n",
        "        return self.loss_function(recon_batch, data, mu, logvar).item(), recon_batch  # sum up batch loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLJeo93BXxjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = VAE(k=50).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "for epoch in range(1, 100 + 1):\n",
        "    vae.trains(device, train_loader, optimizer, epoch)\n",
        "    vae.tests(device, test_loader)\n",
        "    vae.add_embedding(test_loader)\n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTmgcTeAXxjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae.visualize_embeddings(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bzWadNqn2EdJ",
        "colab": {}
      },
      "source": [
        "sample = torch.randn(64, 20).to(device)\n",
        "sample = vae.decode(sample).reshape(64, 28, 28).cpu().detach().numpy()\n",
        "f, axarr = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "  axarr[i//8,i%8].imshow(sample[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnhhJ8F5Xxjv",
        "colab_type": "text"
      },
      "source": [
        "## Stick-breaking process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P-mNDsMXxjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stickbreakingprocess(a, b):\n",
        "    eps = 10*torch.finfo(torch.float).eps\n",
        "    batch_size = a.size()[0]\n",
        "    \n",
        "    uniform_samples = Uniform(torch.tensor([eps]), torch.tensor([1.0-eps])).rsample(a.size()).squeeze().to(device) if not use_cuda else torch.cuda.FloatTensor(a.size(0), a.size(1)).uniform_().clamp(eps, 1.0-eps)\n",
        "    exp_a = torch.reciprocal(a)\n",
        "    exp_b = torch.reciprocal(b)\n",
        "    km = (1- uniform_samples.pow(exp_b) + eps).pow(exp_a)\n",
        "    \n",
        "    #no Nans are allowed in the matrix\n",
        "    #assert not torch.isnan(km).any().item()\n",
        "    \n",
        "    cumprods = torch.cat((torch.ones([batch_size, 1], device=device), torch.cumprod(1-km, axis=1)), dim=1)\n",
        "    sticks = cumprods[:,:-1]*km\n",
        "    sticks[:, -1] = 1- sticks[:, :-1].sum(axis=1) \n",
        "    return sticks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziPKTj5dXxj4",
        "colab_type": "code",
        "outputId": "be037056-4f97-4076-de96-7e7da0f7c520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "stickbreakingprocess(torch.rand(10,20).to(device), torch.rand(10,20).to(device)).sum(axis=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QheULZUTXxkB",
        "colab_type": "text"
      },
      "source": [
        "## Stick-breaking Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qp8w_wAXxkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SBVAE(Autoencoder):\n",
        "    def __init__(self, k):\n",
        "        super(SBVAE, self).__init__()\n",
        "        self.k = k\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, self.k)\n",
        "        self.fc22 = nn.Linear(400, self.k)\n",
        "        \n",
        "        \n",
        "        self.fc3 = nn.Linear(self.k, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "        self.prior_alpha = torch.Tensor([1]).to(device)\n",
        "        self.prior_beta = torch.Tensor([5]).to(device)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return F.softplus(self.fc21(h1)), F.softplus(self.fc22(h1))\n",
        "\n",
        "    def reparameterize(self, a, b):\n",
        "        return stickbreakingprocess(a, b)\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return self.fc4(h3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a, b = self.encode(x)\n",
        "        z = self.reparameterize(a, b)\n",
        "        return self.decode(z), a, b\n",
        "    \n",
        "    def Beta(self, a,b):\n",
        "        return torch.exp(torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a+b))\n",
        "\n",
        "    def KLD(self, a,b, prior_alpha, prior_beta):\n",
        "        ab = (a*b)\n",
        "        kl = 1/(1+ab) * self.Beta(1/a, b)\n",
        "        kl += 1/(2+ab) * self.Beta(2/a, b)\n",
        "        kl += 1/(3+ab) * self.Beta(3/a, b)\n",
        "        kl += 1/(4+ab) * self.Beta(4/a, b)\n",
        "        kl += 1/(5+ab) * self.Beta(5/a, b)\n",
        "        kl += 1/(6+ab) * self.Beta(6/a, b)\n",
        "        kl += 1/(7+ab) * self.Beta(7/a, b)\n",
        "        kl += 1/(8+ab) * self.Beta(8/a, b)\n",
        "        kl += 1/(9+ab) * self.Beta(9/a, b)\n",
        "        kl += 1/(10+ab) * self.Beta(10/a, b)\n",
        "        kl *= (prior_beta-1)*b\n",
        "                                                                                                                                            \n",
        "        kl += (a-prior_alpha)/a * (-np.euler_gamma - torch.digamma(b) - 1/b) #T.psi(self.posterior_b)                                                                                        \n",
        "\n",
        "        # add normalization constants                                                                                                                                                                \n",
        "        kl += torch.log(ab) + torch.log(self.Beta(prior_alpha, prior_beta))\n",
        "\n",
        "        # final term                                                                                                                                                                                 \n",
        "        kl += -(b-1)/b \n",
        "\n",
        "        return kl\n",
        "    \n",
        "    def loss_function(self, recon_x, x, a, b, prior_alpha, prior_beta, epoch, epochs):\n",
        "        period = 20\n",
        "        BCE = F.binary_cross_entropy_with_logits(recon_x, x.view(-1, 784), reduction='none')\n",
        "        KLD = self.KLD(a,b, prior_alpha, prior_beta)\n",
        "\n",
        "        return len(train_loader)/a.size(0) * torch.mean(1/period*(epoch%period)*KLD.sum(axis=1) + BCE.sum(axis=1))\n",
        "    \n",
        "    def compute_loss_train(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, self.prior_alpha, self.prior_beta, epoch, epochs)\n",
        "    \n",
        "    def compute_loss_test(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, self.prior_alpha, self.prior_beta, epoch, epochs).item(), recon_batch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNemMwJXxkI",
        "colab_type": "code",
        "outputId": "d50caf7b-4152-4d2e-b895-1c853adb5c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sbvae = SBVAE(k=50).to(device)\n",
        "optimizer = optim.Adam(sbvae.parameters(), lr=0.003, betas=(0.95, 0.999))\n",
        "sbvae.writer.add_graph(sbvae, next(iter(train_loader))[0].to(device))\n",
        "\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "epochs = 1000\n",
        "for epoch in range(1, epochs + 1): \n",
        "    sbvae.trains(device, train_loader, optimizer, epoch, epochs)\n",
        "    sbvae.tests(device, test_loader, epoch, epochs)\n",
        "    scheduler.step()\n",
        "    sbvae.add_embedding(test_loader)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1007: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[99, 680] (-0.08027458190917969 vs. 0.0394827164709568) and 100264 other locations (99.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2012.844604\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1359.553589\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 881.275574\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 817.419495\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 814.706360\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 752.392090\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 744.263489\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 729.247192\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 732.111694\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 737.104797\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 728.523254\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 712.473999\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 689.507263\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 726.232727\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 719.469666\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 678.057861\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 694.595825\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 691.063904\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 663.538635\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 663.225037\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 669.738220\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 648.052307\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 630.538391\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 649.191101\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 605.659546\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 632.551514\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 624.349792\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 592.461975\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 582.446960\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 604.642151\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 595.529480\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 599.147522\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 589.968628\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 607.461060\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 563.427246\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 597.855713\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 576.764160\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 561.829163\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 582.057922\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 531.874207\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 535.892639\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 560.250122\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 540.292847\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 515.814514\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 519.321167\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 558.713562\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 532.901306\n",
            "\n",
            "Test set: Average loss: 0.0673, Reconstruction error: 0.1414840817451477\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 521.172974\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 548.778809\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 542.623169\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 526.156067\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 526.450439\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 537.637329\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 533.035583\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 547.677551\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 539.695435\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 531.071106\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 526.497742\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 531.126526\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 528.994019\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 518.140137\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 516.635010\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 517.332886\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 546.150879\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 516.962097\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 535.811035\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 499.339935\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 533.638489\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 532.978088\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 530.488892\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 528.057007\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 513.780334\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 519.255066\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 516.008545\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 485.429291\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 523.257263\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 521.280823\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 511.818909\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 519.791870\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 521.406128\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 516.351135\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 530.299438\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 544.541748\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 522.738281\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 520.213989\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 515.277344\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 517.269836\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 516.230286\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 536.735962\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 515.003784\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 534.178589\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 522.267944\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 506.686615\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 552.088196\n",
            "\n",
            "Test set: Average loss: 0.0658, Reconstruction error: 0.1370658129453659\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 553.492981\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 515.965454\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 515.235535\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 528.052185\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 544.007507\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 529.394592\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 507.353638\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 521.698669\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 523.940796\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 530.876587\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 528.528931\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 510.736115\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 512.316528\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 525.721436\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 536.769714\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 500.548523\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 520.671448\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 527.726685\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 531.269897\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 517.751892\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 527.643188\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 517.122437\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 537.305847\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 520.923157\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 516.658875\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 510.180786\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 513.423218\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 514.875671\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 532.569641\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 509.730225\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 511.704987\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 546.367676\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 519.022827\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 529.209351\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 535.304138\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 552.936584\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 506.182678\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 523.486023\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 507.922516\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 501.882080\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 519.597595\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 529.140686\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 508.848572\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 513.092041\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 522.245789\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 519.017090\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 534.424561\n",
            "\n",
            "Test set: Average loss: 0.0663, Reconstruction error: 0.13702140748500824\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 520.488220\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 544.772400\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 516.536377\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 563.608887\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 533.262939\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 513.631409\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 521.140991\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 526.094971\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 519.990417\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 516.088196\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 513.311584\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 518.421570\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 549.317017\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 518.300354\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 536.758545\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 501.518341\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 524.969971\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 534.057861\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 513.329163\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 504.061981\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 558.897095\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 508.558411\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 504.380310\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 540.994446\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 528.761902\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 510.477539\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 509.639984\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 515.587341\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 501.170135\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 536.152954\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 524.974121\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 522.721191\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 533.232910\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 544.082092\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 521.795776\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 554.085510\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 517.828918\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 519.899536\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 508.069977\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 525.433594\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 535.257690\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 506.024170\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 514.370850\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 505.847443\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 534.428162\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 523.270142\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 535.098511\n",
            "\n",
            "Test set: Average loss: 0.0668, Reconstruction error: 0.13698545098304749\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 540.179749\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 524.098206\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 525.812866\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 523.924927\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 531.886230\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 517.445068\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 536.214417\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 524.464661\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 527.980347\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 527.323730\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 527.533447\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 553.513977\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 534.889587\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 538.204590\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 527.741577\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 527.244019\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 565.213440\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 542.665710\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 530.950317\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 537.689575\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 527.786743\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 534.613953\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 524.407776\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 546.297180\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 527.750000\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 544.172791\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 529.647644\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 540.456177\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 541.668091\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 546.394287\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 525.649475\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 522.846863\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 521.102722\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 525.304565\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 527.696899\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 527.997009\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 529.225830\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 536.173035\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 538.427979\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 507.055786\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 520.037354\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 523.009583\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 519.267456\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 517.701477\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 539.867737\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 528.182190\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 509.821106\n",
            "\n",
            "Test set: Average loss: 0.0675, Reconstruction error: 0.1371227204799652\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 525.261658\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 524.802734\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 531.941284\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 527.148987\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 533.831970\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 527.211243\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 548.724426\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 560.375488\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 549.560791\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 543.451294\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 530.176941\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 538.803528\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 518.234619\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 506.636353\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 537.683594\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 523.040771\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 531.773132\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 498.770660\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 537.056458\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 513.761292\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 515.127991\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 515.146301\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 538.687439\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 526.427063\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 530.857727\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 538.457031\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 518.822205\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 536.000549\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 528.146057\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 531.643188\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 532.905151\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 529.732422\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 537.374329\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 556.401001\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 500.101135\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 508.398682\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 527.090271\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 542.662903\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 534.260254\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 539.072632\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 542.174255\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 542.236206\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 539.262268\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 529.709534\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 533.004028\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 505.188599\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 511.635101\n",
            "\n",
            "Test set: Average loss: 0.0682, Reconstruction error: 0.13716447353363037\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 539.460205\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 537.647827\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 534.046814\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 530.542053\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 543.273010\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 551.641907\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 563.003418\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 516.359436\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 541.658813\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 554.543152\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 560.308777\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 533.665161\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 536.659912\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 523.391541\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 564.754944\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 548.012695\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 555.890259\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 525.948181\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 568.429871\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 540.994690\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 550.114990\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 523.054016\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 545.478821\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 539.897949\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 544.005615\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 550.469421\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 536.330505\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 558.640137\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 534.988770\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 551.923706\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 524.496521\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 562.731689\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 560.218994\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 524.328735\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 560.346069\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 537.702637\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 519.176514\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 532.510193\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 533.149841\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 533.086975\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 512.202942\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 536.746399\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 532.954224\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 535.578735\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 514.767212\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 531.980042\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 540.894165\n",
            "\n",
            "Test set: Average loss: 0.0688, Reconstruction error: 0.13707146048545837\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 570.962341\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 553.477844\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 540.211914\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 543.090576\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 540.206787\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 542.395325\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 536.872681\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 542.006287\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 512.540222\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 528.637817\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 525.700500\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 514.419495\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 544.321716\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 563.234375\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 537.437866\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 543.087158\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 539.989868\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 565.595398\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 528.834778\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 553.664673\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 522.273254\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 545.249329\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 539.401062\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 557.193542\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 554.648621\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 534.819397\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 539.520142\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 544.257874\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 539.049194\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 536.587769\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 558.059753\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 547.414673\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 537.623840\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 547.512817\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 523.660034\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 525.858276\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 560.042236\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 545.440552\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 564.241028\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 545.957214\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 553.778564\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 568.291687\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 544.903503\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 547.402100\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 524.350281\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 571.549377\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 551.053833\n",
            "\n",
            "Test set: Average loss: 0.0695, Reconstruction error: 0.13718029856681824\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 564.324341\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 534.666870\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 556.293335\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 574.033630\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 525.924377\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 543.018066\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 560.220459\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 547.357788\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 568.084351\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 551.013123\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 574.943359\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 519.871643\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 550.313965\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 562.170349\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 543.358276\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 539.564270\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 545.245972\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 539.500916\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 573.280518\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 524.781799\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 547.872070\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 570.523438\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 543.797180\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 559.941406\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 541.470947\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 529.795532\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 584.415955\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 555.188416\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 547.774780\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 561.116028\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 554.837036\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 541.686157\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 545.637817\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 542.078552\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 536.782227\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 559.027771\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 539.558472\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 552.363403\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 578.214966\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 577.386414\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 527.012817\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 569.939514\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 516.801147\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 557.558533\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 569.804749\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 555.331543\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 535.114441\n",
            "\n",
            "Test set: Average loss: 0.0701, Reconstruction error: 0.13709492981433868\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 539.009216\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 556.757263\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 567.029968\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 543.629578\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 552.713989\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 544.078186\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 563.245056\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 551.784119\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 557.421143\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 566.901794\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 556.479919\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 565.349243\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 551.593811\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 557.911011\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 563.916321\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 566.331116\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 582.147278\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 568.097107\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 555.217957\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 540.539429\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 583.464478\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 534.828125\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 559.024475\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 544.887939\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 541.343567\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 551.544189\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 547.828857\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 561.636414\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 571.370239\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 547.838196\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 579.868713\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 544.420471\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 566.851562\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 553.386292\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 560.172546\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 545.598389\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 554.829041\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 546.676941\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 552.322754\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 573.060791\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 543.897644\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 536.488281\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 562.246887\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 548.486755\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 558.814453\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 575.291443\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 573.366211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-aae4c358b675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-850d1fd2912e>\u001b[0m in \u001b[0;36mtrains\u001b[0;34m(self, device, train_loader, optimizer, epoch, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5aDfJvHXxkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sbvae.visualize_embeddings(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJstAFVO7Lls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.distributions.beta import Beta\n",
        "sample = Beta(torch.tensor([1.0]), torch.tensor([5.0])).rsample([64,50]).squeeze().to(device)\n",
        "cumprods = torch.cat((torch.ones([64, 1], device=device), torch.cumprod(1-sample, axis=1)), dim=1)\n",
        "sample = cumprods[:,:-1]*sample\n",
        "sample[:, -1] = 1- sample[:, :-1].sum(axis=1)\n",
        "sample = torch.sigmoid(sbvae.decode(sample)).reshape(64, 28, 28).cpu().detach().numpy()\n",
        "f, axarr = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "  axarr[i//8,i%8].imshow(sample[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA5Txpn2Djld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSSBVAE(SBVAE):\n",
        "    def __init__(self, k):\n",
        "        super(SSSBVAE, self).__init__(k)\n",
        "  \n",
        "        self.fc23 = nn.Linear(400, 10)\n",
        "        self.fc3 = nn.Linear(self.k + 10, 400)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return F.softplus(self.fc21(h1)), F.softplus(self.fc22(h1)), F.softplus(self.fc23(h1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        a, b, y = self.encode(x)\n",
        "        z = self.reparameterize(a, b)\n",
        "        z = torch.cat((z, y/torch.norm(y, p=1)),dim=1)\n",
        "        return self.decode(z), a, b, y/torch.norm(y, p=1)\n",
        "    \n",
        "    def loss_function(self, recon_x, x, a, b, y, y_true, prior_alpha, prior_beta, epoch, epochs):\n",
        "        period = 20\n",
        "        BCE = F.binary_cross_entropy_with_logits(recon_x, x.view(-1, 784), reduction='none')\n",
        "        KLD = self.KLD(a,b, prior_alpha, prior_beta)\n",
        "\n",
        "        log_y = F.binary_cross_entropy(y, torch.eye(10, device=device)[y_true], reduction='none').sum(axis=1)\n",
        "        ent_y = Categorical(probs=y).entropy()\n",
        "\n",
        "        y_recon = torch.where(y_true !=-1, log_y, ent_y)\n",
        "\n",
        "        eye_batch = torch.eye(10, device=device).unsqueeze(0).expand(a.size(0), -1, -1)\n",
        "        y_batch = y.unsqueeze(2).expand(-1, -1, 10)\n",
        "\n",
        "        factor = torch.where(y_true !=-1, torch.ones(a.size(0), device=device), F.binary_cross_entropy(y_batch, eye_batch, reduction='none').sum(axis=(1,2))) \n",
        "\n",
        "        term = 1/period*(epoch%period)*KLD.sum(axis=1) + BCE.sum(axis=1)*factor + y_recon\n",
        "\n",
        "        return len(train_loader)/a.size(0) * (torch.mean(term))\n",
        "\n",
        "    def compute_loss_train(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b, y = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, y, target, self.prior_alpha, self.prior_beta, epoch, epochs)\n",
        "    \n",
        "    def compute_loss_test(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b, y = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, y, target, self.prior_alpha, self.prior_beta, epoch, epochs).item(), recon_batch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdJtNpRCiIUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "fca5ca1c-17bd-4d5f-d3dd-73686fb6db95"
      },
      "source": [
        "sssbvae = SSSBVAE(k=50).to(device)\n",
        "optimizer = optim.Adam(sssbvae.parameters(), lr=0.003, betas=(0.95, 0.999))\n",
        "sssbvae.writer.add_graph(sssbvae, next(iter(train_loader_occluded))[0].to(device))\n",
        "\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "epochs = 1000\n",
        "for epoch in range(1, epochs + 1): \n",
        "    sssbvae.trains(device, train_loader_occluded, optimizer, epoch, epochs)\n",
        "    sssbvae.tests(device, test_loader_occluded, epoch, epochs)\n",
        "    scheduler.step()\n",
        "    #sssbvae.add_embedding(test_loader)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1007: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[13, 110] (-0.1163659542798996 vs. -0.0008146166801452637) and 100218 other locations (99.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-22d0cdcc9bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msssbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_occluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msssbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_occluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-850d1fd2912e>\u001b[0m in \u001b[0;36mtrains\u001b[0;34m(self, device, train_loader, optimizer, epoch, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-112-0b0927e9e278>\u001b[0m in \u001b[0;36mcompute_loss_train\u001b[0;34m(self, data, target, epoch, epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-112-0b0927e9e278>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(self, recon_x, x, a, b, y, y_true, prior_alpha, prior_beta, epoch, epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meye_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mterm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mKLD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBCE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfactor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_recon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: where(): argument 'input' (position 2) must be Tensor, not int"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Hpkmer3wEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "07b72ede-8eb4-4b9a-c1f6-e138b3e17659"
      },
      "source": [
        "sample = Beta(torch.tensor([1.0]), torch.tensor([5.0])).rsample([64,50]).squeeze().to(device)\n",
        "nums = np.random.randint(10, size=64)\n",
        "onehot = torch.eye(10, device=device)[nums]\n",
        "\n",
        "cumprods = torch.cat((torch.ones([64, 1], device=device), torch.cumprod(1-sample, axis=1)), dim=1)\n",
        "sample = cumprods[:,:-1]*sample\n",
        "sample[:, -1] = 1- sample[:, :-1].sum(axis=1)\n",
        "print(nums)\n",
        "inputs = torch.cat((sample, onehot), dim=1)\n",
        "sample = torch.sigmoid(sssbvae.decode(inputs)).reshape(64, 28, 28).cpu().detach().numpy()\n",
        "f, axarr = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "  axarr[i//8,i%8].imshow(sample[i])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6 5 4 3 5 0 0 7 6 0 7 1 0 1 4 8 9 2 5 3 3 4 6 0 4 2 2 7 2 2 8 1 7 5 6 3 0\n",
            " 0 8 0 8 2 2 6 9 5 4 6 7 5 3 8 7 6 4 7 3 3 8 6 3 7 2 6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAD8CAYAAABaZT40AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnmIdtGEYRp7yLxXTk3F7Ds4HAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 64 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfmG5q9g0ggp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /data/runs/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}