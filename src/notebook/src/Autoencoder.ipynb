{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Autoencoder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wittenator/pimai/blob/master/src/notebook/src/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90gWsaxMXxiK",
        "colab_type": "code",
        "outputId": "a8231896-cecb-4544-f351-4ce4997ea027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  !pip install torch torchvision skorch\n",
        "  !pip install hypertools\n",
        "  colab = True\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Collecting skorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/1e/cc4e1f23cd1faab06672f309e0857294aaa80c5f84670f4d3d19b08ab10b/skorch-0.7.0-py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.7.0\n",
            "Collecting hypertools\n",
            "  Downloading https://files.pythonhosted.org/packages/74/85/94f7f6908646fe19fbd36dbcab7e5a5861255f64f4629367dbc52b538a36/hypertools-0.6.2.tar.gz\n",
            "Collecting PPCA>=0.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/16/7f/7195bf3742e19076a21a9c3250a4f11c87153bb4ea3dcaf1077678383b76/ppca-0.0.4-py3-none-any.whl\n",
            "Collecting scikit-learn<0.22,>=0.19.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from hypertools) (0.25.3)\n",
            "Requirement already satisfied: seaborn>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from hypertools) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from hypertools) (3.1.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from hypertools) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from hypertools) (1.17.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hypertools) (0.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from hypertools) (2.21.0)\n",
            "Collecting deepdish\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/39/2a47c852651982bc5eb39212ac110284dd20126bdc7b49bde401a0139f5d/deepdish-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hypertools) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22,>=0.19.1->hypertools) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->hypertools) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->hypertools) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->hypertools) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->hypertools) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->hypertools) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hypertools) (2.8)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from deepdish->hypertools) (3.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->hypertools) (42.0.2)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->deepdish->hypertools) (2.7.1)\n",
            "Building wheels for collected packages: hypertools\n",
            "  Building wheel for hypertools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hypertools: filename=hypertools-0.6.2-cp36-none-any.whl size=46622 sha256=2c7f32ebeda3f237d46fb147233ca6b474dae26056ab5b7ed75cbb98e49640a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/31/3a/0a3f26ae77857ae19ee5947fd2ee9bf34249fcd9c6c90636c2\n",
            "Successfully built hypertools\n",
            "Installing collected packages: PPCA, scikit-learn, deepdish, hypertools\n",
            "  Found existing installation: scikit-learn 0.22.1\n",
            "    Uninstalling scikit-learn-0.22.1:\n",
            "      Successfully uninstalled scikit-learn-0.22.1\n",
            "Successfully installed PPCA-0.0.4 deepdish-0.3.6 hypertools-0.6.2 scikit-learn-0.21.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TPMgseDXxiZ",
        "colab_type": "code",
        "outputId": "42b9595d-b464-49f8-88f0-19cdd3d9d68d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from datetime import datetime\n",
        "from scipy.signal import sawtooth\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "from torch.distributions import *\n",
        "\n",
        "import skorch\n",
        "import numpy as np\n",
        "import hypertools as hyp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%load_ext tensorboard\n",
        "%matplotlib inline\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f6a0c35c0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaYNKNJqXxip",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koWH_sFdXxit",
        "colab_type": "code",
        "outputId": "eeeb1337-a875-4db3-9d53-8cd05679cc1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': cpu_count(), 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "mnist_train = datasets.MNIST('/data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "mnist_test = datasets.MNIST('/data', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "print(int(len(mnist_train)*prob))\n",
        "prob = 0.05\n",
        "train_occluded = np.array([1] * int(len(mnist_train)*prob) + [0] * int((1-prob)*len(mnist_train)))\n",
        "test_occluded = np.array([1] * int(len(mnist_test)*prob) + [0] * int((1-prob)*len(mnist_test)))\n",
        "np.random.shuffle(train_occluded)\n",
        "np.random.shuffle(test_occluded)\n",
        "\n",
        "\n",
        "mnist_train_occluded =datasets.MNIST('/data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "mnist_train_occluded.targets[train_occluded] = -1\n",
        "mnist_test_occluded = datasets.MNIST('/data', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "mnist_test_occluded.targets[test_occluded] = -1\n",
        "\n",
        "\n",
        "if not colab:\n",
        "  train_loader = DataLoader(Subset(mnist_train, indices=range(1000)), batch_size=64, shuffle=True, **kwargs)\n",
        "  test_loader = DataLoader(Subset(mnist_test, indices=range(1000)), batch_size=1000, shuffle=True, **kwargs)\n",
        "else:\n",
        "  train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True, **kwargs)\n",
        "  test_loader = DataLoader(mnist_test, batch_size=1000, shuffle=True, **kwargs)\n",
        "  \n",
        "  train_loader_occluded = DataLoader(mnist_train_occluded, batch_size=128, shuffle=True, **kwargs)\n",
        "  test_loader_occluded = DataLoader(mnist_test_occluded, batch_size=1000, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbmJnimKXxi3",
        "colab_type": "text"
      },
      "source": [
        "## Generic autoencoder class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vnseljiXxi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        now = datetime.now()\n",
        "        current_time = now.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        self.writer = SummaryWriter(log_dir=\"/data/runs/\"+current_time)\n",
        "        self.embeddings = []\n",
        "        self.embedding_labels =[]\n",
        "    \n",
        "    def trains(self, device, train_loader, optimizer, epoch, epochs):\n",
        "        self.train()\n",
        "        loss_sum = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = self.compute_loss_train(data, target, epoch, epochs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "            self.writer.add_scalar('Loss/train', loss.item(), epoch*len(train_loader)+batch_idx)\n",
        "            \n",
        "    def tests(self, device, test_loader, epoch, epochs):\n",
        "        self.eval()\n",
        "        test_loss = 0\n",
        "        recon = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                loss, output = self.compute_loss_test(data, target, epoch, epochs)\n",
        "                test_loss += loss\n",
        "                recon += F.binary_cross_entropy_with_logits(output, data.view(-1, 784), reduction='none').sum(axis=1).mean()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        recon /= len(test_loader.dataset)\n",
        "\n",
        "        print('\\nTest set: Average loss: {:.4f}, Reconstruction error: {}\\n'.format(\n",
        "            test_loss, recon))\n",
        "        \n",
        "    def add_embedding(self, loader):\n",
        "        with torch.no_grad():\n",
        "            labels = []\n",
        "            embs = []\n",
        "            for data, label in loader:\n",
        "                data, label = data.to(device), label.to(device)\n",
        "                labels.append(label)\n",
        "                recon_batch, a, b = self(data)\n",
        "                emb = self.reparameterize(a,b)\n",
        "                embs.append(emb)\n",
        "            self.embeddings.append(torch.cat(tuple(embs), dim=0).cpu().numpy())\n",
        "            self.embedding_labels = torch.cat(tuple(labels), dim=0).cpu().numpy()\n",
        "            \n",
        "    def visualize_embeddings(self, epoch):\n",
        "        hyp.plot(self.embeddings[epoch], '.', hue=self.embedding_labels, reduce='TSNE', ndims=2, save_path=f'/data/visualizations/{self.__class__.__name__}-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.svg' if not colab else None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca_SLALTXxjC",
        "colab_type": "text"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qKCWvCFXxjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleAutoencoder(Autoencoder):\n",
        "    def __init__(self):\n",
        "        super(SimpleAutoencoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output       \n",
        "    \n",
        "    def compute_loss_train(self, data, target):\n",
        "        output = self(data)\n",
        "        return F.nll_loss(output, target)\n",
        "    \n",
        "    def compute_loss_test(self, data, target):\n",
        "        output = self(data)\n",
        "        return F.nll_loss(output, target, reduction='sum').item(), output  # sum up batch loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTS4EVR9XxjN",
        "colab_type": "code",
        "outputId": "942355ed-9658-4083-8493-34ae556241dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "model = SimpleAutoencoder().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters())\n",
        "\n",
        "# plot model\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "# write to tensorboard\n",
        "#writer.add_image('mnist_images', img_grid)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "for epoch in range(1, 14 + 1):\n",
        "    model.trains(device, train_loader, optimizer, epoch)\n",
        "    model.tests(device, test_loader)\n",
        "    scheduler.step()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2eebc719d3ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: trains() missing 1 required positional argument: 'epochs'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWmNtPJBXxjS",
        "colab_type": "text"
      },
      "source": [
        "## Gaussian Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz2yfQy7XxjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(Autoencoder):\n",
        "    def __init__(self, k=20):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, k)\n",
        "        self.fc22 = nn.Linear(400, k)\n",
        "        self.fc3 = nn.Linear(k, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        #x = self.conv1(x)\n",
        "        #x = F.relu(x)\n",
        "        #x = self.conv2(x)\n",
        "        #x = F.max_pool2d(x, 2)\n",
        "        #x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "    \n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "        # see Appendix B from VAE paper:\n",
        "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "        # https://arxiv.org/abs/1312.6114\n",
        "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        return BCE + KLD\n",
        "    \n",
        "    def compute_loss_train(self, data, target, epoch, epochs):\n",
        "        recon_batch, mu, logvar = self(data)\n",
        "        return self.loss_function(recon_batch, data, mu, logvar)\n",
        "    \n",
        "    def compute_loss_test(self, data, target):\n",
        "        recon_batch, mu, logvar = self(data)\n",
        "        return self.loss_function(recon_batch, data, mu, logvar).item(), recon_batch  # sum up batch loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLJeo93BXxjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = VAE(k=50).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "for epoch in range(1, 100 + 1):\n",
        "    vae.trains(device, train_loader, optimizer, epoch)\n",
        "    vae.tests(device, test_loader)\n",
        "    vae.add_embedding(test_loader)\n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTmgcTeAXxjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae.visualize_embeddings(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bzWadNqn2EdJ",
        "colab": {}
      },
      "source": [
        "sample = torch.randn(64, 20).to(device)\n",
        "sample = vae.decode(sample).reshape(64, 28, 28).cpu().detach().numpy()\n",
        "f, axarr = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "  axarr[i//8,i%8].imshow(sample[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnhhJ8F5Xxjv",
        "colab_type": "text"
      },
      "source": [
        "## Stick-breaking process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P-mNDsMXxjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stickbreakingprocess(a, b):\n",
        "    eps = 10*torch.finfo(torch.float).eps\n",
        "    batch_size = a.size()[0]\n",
        "    \n",
        "    uniform_samples = Uniform(torch.tensor([eps]), torch.tensor([1.0-eps])).rsample(a.size()).squeeze().to(device) if not use_cuda else torch.cuda.FloatTensor(a.size(0), a.size(1)).uniform_().clamp(eps, 1.0-eps)\n",
        "    exp_a = torch.reciprocal(a)\n",
        "    exp_b = torch.reciprocal(b)\n",
        "    km = (1- uniform_samples.pow(exp_b) + eps).pow(exp_a)\n",
        "    \n",
        "    #no Nans are allowed in the matrix\n",
        "    #assert not torch.isnan(km).any().item()\n",
        "    \n",
        "    cumprods = torch.cat((torch.ones([batch_size, 1], device=device), torch.cumprod(1-km, axis=1)), dim=1)\n",
        "    sticks = cumprods[:,:-1]*km\n",
        "    sticks[:, -1] = 1- sticks[:, :-1].sum(axis=1) \n",
        "    return sticks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziPKTj5dXxj4",
        "colab_type": "code",
        "outputId": "be037056-4f97-4076-de96-7e7da0f7c520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "stickbreakingprocess(torch.rand(10,20).to(device), torch.rand(10,20).to(device)).sum(axis=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QheULZUTXxkB",
        "colab_type": "text"
      },
      "source": [
        "## Stick-breaking Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qp8w_wAXxkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SBVAE(Autoencoder):\n",
        "    def __init__(self, k):\n",
        "        super(SBVAE, self).__init__()\n",
        "        self.k = k\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, self.k)\n",
        "        self.fc22 = nn.Linear(400, self.k)\n",
        "        \n",
        "        \n",
        "        self.fc3 = nn.Linear(self.k, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "        self.prior_alpha = torch.Tensor([1]).to(device)\n",
        "        self.prior_beta = torch.Tensor([5]).to(device)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return F.softplus(self.fc21(h1)), F.softplus(self.fc22(h1))\n",
        "\n",
        "    def reparameterize(self, a, b):\n",
        "        return stickbreakingprocess(a, b)\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return self.fc4(h3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a, b = self.encode(x)\n",
        "        z = self.reparameterize(a, b)\n",
        "        return self.decode(z), a, b\n",
        "    \n",
        "    def Beta(self, a,b):\n",
        "        return torch.exp(torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a+b))\n",
        "\n",
        "    def KLD(self, a,b, prior_alpha, prior_beta):\n",
        "        ab = (a*b)\n",
        "        kl = 1/(1+ab) * self.Beta(1/a, b)\n",
        "        kl += 1/(2+ab) * self.Beta(2/a, b)\n",
        "        kl += 1/(3+ab) * self.Beta(3/a, b)\n",
        "        kl += 1/(4+ab) * self.Beta(4/a, b)\n",
        "        kl += 1/(5+ab) * self.Beta(5/a, b)\n",
        "        kl += 1/(6+ab) * self.Beta(6/a, b)\n",
        "        kl += 1/(7+ab) * self.Beta(7/a, b)\n",
        "        kl += 1/(8+ab) * self.Beta(8/a, b)\n",
        "        kl += 1/(9+ab) * self.Beta(9/a, b)\n",
        "        kl += 1/(10+ab) * self.Beta(10/a, b)\n",
        "        kl *= (prior_beta-1)*b\n",
        "                                                                                                                                            \n",
        "        kl += (a-prior_alpha)/a * (-np.euler_gamma - torch.digamma(b) - 1/b) #T.psi(self.posterior_b)                                                                                        \n",
        "\n",
        "        # add normalization constants                                                                                                                                                                \n",
        "        kl += torch.log(ab) + torch.log(self.Beta(prior_alpha, prior_beta))\n",
        "\n",
        "        # final term                                                                                                                                                                                 \n",
        "        kl += -(b-1)/b \n",
        "\n",
        "        return kl\n",
        "    \n",
        "    def loss_function(self, recon_x, x, a, b, prior_alpha, prior_beta, epoch, epochs):\n",
        "        period = 20\n",
        "        BCE = F.binary_cross_entropy_with_logits(recon_x, x.view(-1, 784), reduction='none')\n",
        "        KLD = self.KLD(a,b, prior_alpha, prior_beta)\n",
        "\n",
        "        return len(train_loader)/a.size(0) * torch.mean(1/period*(epoch%period)*KLD.sum(axis=1) + BCE.sum(axis=1))\n",
        "    \n",
        "    def compute_loss_train(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, self.prior_alpha, self.prior_beta, epoch, epochs)\n",
        "    \n",
        "    def compute_loss_test(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, self.prior_alpha, self.prior_beta, epoch, epochs).item(), recon_batch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNemMwJXxkI",
        "colab_type": "code",
        "outputId": "d50caf7b-4152-4d2e-b895-1c853adb5c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sbvae = SBVAE(k=50).to(device)\n",
        "optimizer = optim.Adam(sbvae.parameters(), lr=0.003, betas=(0.95, 0.999))\n",
        "sbvae.writer.add_graph(sbvae, next(iter(train_loader))[0].to(device))\n",
        "\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "epochs = 1000\n",
        "for epoch in range(1, epochs + 1): \n",
        "    sbvae.trains(device, train_loader, optimizer, epoch, epochs)\n",
        "    sbvae.tests(device, test_loader, epoch, epochs)\n",
        "    scheduler.step()\n",
        "    sbvae.add_embedding(test_loader)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1007: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[99, 680] (-0.08027458190917969 vs. 0.0394827164709568) and 100264 other locations (99.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2012.844604\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1359.553589\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 881.275574\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 817.419495\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 814.706360\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 752.392090\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 744.263489\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 729.247192\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 732.111694\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 737.104797\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 728.523254\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 712.473999\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 689.507263\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 726.232727\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 719.469666\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 678.057861\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 694.595825\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 691.063904\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 663.538635\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 663.225037\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 669.738220\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 648.052307\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 630.538391\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 649.191101\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 605.659546\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 632.551514\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 624.349792\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 592.461975\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 582.446960\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 604.642151\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 595.529480\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 599.147522\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 589.968628\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 607.461060\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 563.427246\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 597.855713\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 576.764160\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 561.829163\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 582.057922\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 531.874207\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 535.892639\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 560.250122\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 540.292847\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 515.814514\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 519.321167\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 558.713562\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 532.901306\n",
            "\n",
            "Test set: Average loss: 0.0673, Reconstruction error: 0.1414840817451477\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 521.172974\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 548.778809\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 542.623169\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 526.156067\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 526.450439\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 537.637329\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 533.035583\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 547.677551\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 539.695435\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 531.071106\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 526.497742\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 531.126526\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 528.994019\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 518.140137\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 516.635010\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 517.332886\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 546.150879\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 516.962097\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 535.811035\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 499.339935\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 533.638489\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 532.978088\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 530.488892\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 528.057007\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 513.780334\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 519.255066\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 516.008545\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 485.429291\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 523.257263\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 521.280823\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 511.818909\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 519.791870\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 521.406128\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 516.351135\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 530.299438\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 544.541748\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 522.738281\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 520.213989\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 515.277344\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 517.269836\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 516.230286\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 536.735962\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 515.003784\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 534.178589\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 522.267944\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 506.686615\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 552.088196\n",
            "\n",
            "Test set: Average loss: 0.0658, Reconstruction error: 0.1370658129453659\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 553.492981\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 515.965454\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 515.235535\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 528.052185\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 544.007507\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 529.394592\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 507.353638\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 521.698669\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 523.940796\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 530.876587\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 528.528931\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 510.736115\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 512.316528\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 525.721436\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 536.769714\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 500.548523\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 520.671448\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 527.726685\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 531.269897\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 517.751892\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 527.643188\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 517.122437\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 537.305847\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 520.923157\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 516.658875\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 510.180786\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 513.423218\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 514.875671\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 532.569641\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 509.730225\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 511.704987\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 546.367676\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 519.022827\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 529.209351\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 535.304138\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 552.936584\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 506.182678\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 523.486023\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 507.922516\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 501.882080\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 519.597595\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 529.140686\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 508.848572\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 513.092041\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 522.245789\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 519.017090\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 534.424561\n",
            "\n",
            "Test set: Average loss: 0.0663, Reconstruction error: 0.13702140748500824\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 520.488220\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 544.772400\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 516.536377\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 563.608887\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 533.262939\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 513.631409\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 521.140991\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 526.094971\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 519.990417\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 516.088196\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 513.311584\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 518.421570\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 549.317017\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 518.300354\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 536.758545\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 501.518341\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 524.969971\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 534.057861\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 513.329163\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 504.061981\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 558.897095\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 508.558411\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 504.380310\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 540.994446\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 528.761902\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 510.477539\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 509.639984\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 515.587341\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 501.170135\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 536.152954\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 524.974121\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 522.721191\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 533.232910\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 544.082092\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 521.795776\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 554.085510\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 517.828918\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 519.899536\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 508.069977\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 525.433594\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 535.257690\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 506.024170\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 514.370850\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 505.847443\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 534.428162\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 523.270142\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 535.098511\n",
            "\n",
            "Test set: Average loss: 0.0668, Reconstruction error: 0.13698545098304749\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 540.179749\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 524.098206\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 525.812866\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 523.924927\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 531.886230\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 517.445068\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 536.214417\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 524.464661\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 527.980347\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 527.323730\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 527.533447\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 553.513977\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 534.889587\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 538.204590\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 527.741577\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 527.244019\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 565.213440\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 542.665710\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 530.950317\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 537.689575\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 527.786743\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 534.613953\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 524.407776\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 546.297180\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 527.750000\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 544.172791\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 529.647644\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 540.456177\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 541.668091\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 546.394287\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 525.649475\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 522.846863\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 521.102722\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 525.304565\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 527.696899\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 527.997009\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 529.225830\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 536.173035\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 538.427979\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 507.055786\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 520.037354\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 523.009583\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 519.267456\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 517.701477\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 539.867737\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 528.182190\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 509.821106\n",
            "\n",
            "Test set: Average loss: 0.0675, Reconstruction error: 0.1371227204799652\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 525.261658\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 524.802734\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 531.941284\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 527.148987\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 533.831970\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 527.211243\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 548.724426\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 560.375488\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 549.560791\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 543.451294\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 530.176941\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 538.803528\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 518.234619\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 506.636353\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 537.683594\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 523.040771\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 531.773132\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 498.770660\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 537.056458\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 513.761292\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 515.127991\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 515.146301\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 538.687439\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 526.427063\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 530.857727\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 538.457031\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 518.822205\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 536.000549\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 528.146057\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 531.643188\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 532.905151\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 529.732422\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 537.374329\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 556.401001\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 500.101135\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 508.398682\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 527.090271\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 542.662903\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 534.260254\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 539.072632\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 542.174255\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 542.236206\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 539.262268\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 529.709534\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 533.004028\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 505.188599\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 511.635101\n",
            "\n",
            "Test set: Average loss: 0.0682, Reconstruction error: 0.13716447353363037\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 539.460205\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 537.647827\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 534.046814\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 530.542053\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 543.273010\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 551.641907\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 563.003418\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 516.359436\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 541.658813\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 554.543152\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 560.308777\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 533.665161\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 536.659912\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 523.391541\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 564.754944\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 548.012695\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 555.890259\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 525.948181\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 568.429871\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 540.994690\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 550.114990\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 523.054016\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 545.478821\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 539.897949\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 544.005615\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 550.469421\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 536.330505\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 558.640137\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 534.988770\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 551.923706\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 524.496521\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 562.731689\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 560.218994\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 524.328735\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 560.346069\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 537.702637\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 519.176514\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 532.510193\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 533.149841\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 533.086975\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 512.202942\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 536.746399\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 532.954224\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 535.578735\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 514.767212\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 531.980042\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 540.894165\n",
            "\n",
            "Test set: Average loss: 0.0688, Reconstruction error: 0.13707146048545837\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 570.962341\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 553.477844\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 540.211914\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 543.090576\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 540.206787\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 542.395325\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 536.872681\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 542.006287\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 512.540222\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 528.637817\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 525.700500\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 514.419495\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 544.321716\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 563.234375\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 537.437866\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 543.087158\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 539.989868\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 565.595398\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 528.834778\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 553.664673\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 522.273254\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 545.249329\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 539.401062\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 557.193542\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 554.648621\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 534.819397\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 539.520142\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 544.257874\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 539.049194\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 536.587769\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 558.059753\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 547.414673\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 537.623840\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 547.512817\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 523.660034\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 525.858276\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 560.042236\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 545.440552\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 564.241028\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 545.957214\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 553.778564\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 568.291687\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 544.903503\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 547.402100\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 524.350281\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 571.549377\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 551.053833\n",
            "\n",
            "Test set: Average loss: 0.0695, Reconstruction error: 0.13718029856681824\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 564.324341\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 534.666870\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 556.293335\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 574.033630\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 525.924377\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 543.018066\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 560.220459\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 547.357788\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 568.084351\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 551.013123\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 574.943359\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 519.871643\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 550.313965\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 562.170349\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 543.358276\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 539.564270\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 545.245972\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 539.500916\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 573.280518\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 524.781799\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 547.872070\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 570.523438\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 543.797180\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 559.941406\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 541.470947\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 529.795532\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 584.415955\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 555.188416\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 547.774780\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 561.116028\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 554.837036\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 541.686157\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 545.637817\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 542.078552\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 536.782227\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 559.027771\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 539.558472\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 552.363403\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 578.214966\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 577.386414\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 527.012817\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 569.939514\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 516.801147\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 557.558533\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 569.804749\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 555.331543\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 535.114441\n",
            "\n",
            "Test set: Average loss: 0.0701, Reconstruction error: 0.13709492981433868\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 539.009216\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 556.757263\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 567.029968\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 543.629578\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 552.713989\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 544.078186\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 563.245056\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 551.784119\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 557.421143\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 566.901794\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 556.479919\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 565.349243\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 551.593811\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 557.911011\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 563.916321\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 566.331116\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 582.147278\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 568.097107\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 555.217957\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 540.539429\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 583.464478\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 534.828125\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 559.024475\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 544.887939\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 541.343567\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 551.544189\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 547.828857\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 561.636414\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 571.370239\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 547.838196\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 579.868713\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 544.420471\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 566.851562\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 553.386292\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 560.172546\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 545.598389\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 554.829041\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 546.676941\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 552.322754\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 573.060791\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 543.897644\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 536.488281\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 562.246887\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 548.486755\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 558.814453\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 575.291443\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 573.366211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-aae4c358b675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msbvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-850d1fd2912e>\u001b[0m in \u001b[0;36mtrains\u001b[0;34m(self, device, train_loader, optimizer, epoch, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5aDfJvHXxkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sbvae.visualize_embeddings(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJstAFVO7Lls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.distributions.beta import Beta\n",
        "sample = Beta(torch.tensor([1.0]), torch.tensor([5.0])).rsample([64,50]).squeeze().to(device)\n",
        "cumprods = torch.cat((torch.ones([64, 1], device=device), torch.cumprod(1-sample, axis=1)), dim=1)\n",
        "sample = cumprods[:,:-1]*sample\n",
        "sample[:, -1] = 1- sample[:, :-1].sum(axis=1)\n",
        "sample = torch.sigmoid(sbvae.decode(sample)).reshape(64, 28, 28).cpu().detach().numpy()\n",
        "f, axarr = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "  axarr[i//8,i%8].imshow(sample[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA5Txpn2Djld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSSBVAE(SBVAE):\n",
        "    def __init__(self, k):\n",
        "        super(SSSBVAE, self).__init__(k)\n",
        "  \n",
        "        self.fc23 = nn.Linear(400, 10)\n",
        "        self.fc3 = nn.Linear(self.k + 10, 400)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return F.softplus(self.fc21(h1)), F.softplus(self.fc22(h1)), F.softplus(self.fc23(h1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        a, b, y = self.encode(x)\n",
        "        z = self.reparameterize(a, b)\n",
        "        z = torch.cat((z, y/torch.norm(y, p=1)),dim=1)\n",
        "        return self.decode(z), a, b, y/torch.norm(y, p=1)\n",
        "    \n",
        "    def loss_function(self, recon_x, x, a, b, y, y_true, prior_alpha, prior_beta, epoch, epochs):\n",
        "        period = 20\n",
        "        BCE = F.binary_cross_entropy_with_logits(recon_x, x.view(-1, 784), reduction='none')\n",
        "        KLD = self.KLD(a,b, prior_alpha, prior_beta)\n",
        "\n",
        "        log_y = F.binary_cross_entropy(y, torch.eye(10, device=device)[y_true], reduction='none').sum(axis=1)\n",
        "        ent_y = -Categorical(probs=y).entropy()\n",
        "\n",
        "        y_recon = torch.where(y_true !=-1, log_y, ent_y)\n",
        "\n",
        "        eye_batch = torch.eye(10, device=device).unsqueeze(0).expand(a.size(0), -1, -1)\n",
        "        y_batch = y.unsqueeze(2).expand(-1, -1, 10)\n",
        "\n",
        "        factor = torch.where(y_true !=-1, torch.ones(a.size(0), device=device), F.binary_cross_entropy(y_batch, eye_batch, reduction='none').sum(axis=(1,2))) \n",
        "\n",
        "        term = 1/period*(epoch%period)*KLD.sum(axis=1) + BCE.sum(axis=1)*factor + y_recon\n",
        "\n",
        "        return len(train_loader)/a.size(0) * (torch.mean(term))\n",
        "\n",
        "    def compute_loss_train(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b, y = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, y, target, self.prior_alpha, self.prior_beta, epoch, epochs)\n",
        "    \n",
        "    def compute_loss_test(self, data, target, epoch, epochs):\n",
        "        recon_batch, a, b, y = self(data)\n",
        "        return self.loss_function(recon_batch, data, a, b, y, target, self.prior_alpha, self.prior_beta, epoch, epochs).item(), recon_batch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdJtNpRCiIUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ca65bc3-fb91-4b7e-985e-6da009e28890"
      },
      "source": [
        "sssbvae = SSSBVAE(k=50).to(device)\n",
        "optimizer = optim.Adam(sssbvae.parameters(), lr=0.003, betas=(0.95, 0.999))\n",
        "sssbvae.writer.add_graph(sssbvae, next(iter(train_loader_occluded))[0].to(device))\n",
        "\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "epochs = 1000\n",
        "for epoch in range(1, epochs + 1): \n",
        "    sssbvae.trains(device, train_loader_occluded, optimizer, epoch, epochs)\n",
        "    sssbvae.tests(device, test_loader_occluded, epoch, epochs)\n",
        "    scheduler.step()\n",
        "    #sssbvae.add_embedding(test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1007: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 730] (-0.07831178605556488 vs. 0.06194216385483742) and 100261 other locations (99.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2038.397949\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1344.346558\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 893.952637\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 810.775452\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 805.439026\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 823.371155\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 769.430054\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 777.004700\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 729.300659\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 725.679749\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 728.231201\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 714.262939\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 710.365784\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 697.983582\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 701.537903\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 666.210266\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 717.100708\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 660.669312\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 659.267090\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 653.210876\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 634.389465\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 636.593201\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 661.671997\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 614.558777\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 626.906372\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 627.098816\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 610.283508\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 608.191833\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 594.658203\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 592.833618\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 597.075989\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 583.212463\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 562.989929\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 578.941772\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 570.036133\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 573.101868\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 549.470398\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 562.364807\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 563.349487\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 558.732178\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 558.025818\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 530.964355\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 531.456787\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 550.174194\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 528.187256\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 530.428345\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 533.224426\n",
            "\n",
            "Test set: Average loss: 0.0701, Reconstruction error: 0.13635626435279846\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 529.199036\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 518.078979\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 533.081909\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 532.063782\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 524.972717\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 531.332031\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 529.097412\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 526.726868\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 520.779053\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 519.579590\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 509.918915\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 512.227905\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 531.455750\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 549.942993\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 520.379395\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 524.090454\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 529.859558\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 513.290955\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 505.272522\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 528.472229\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 522.529724\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 523.751831\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 495.096710\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 514.173462\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 512.906128\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 507.990997\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 519.328064\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 524.017090\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 525.074829\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 517.455994\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 510.886017\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 525.225708\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 501.516418\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 533.745789\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 503.411530\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 481.053131\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 501.394226\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 513.495850\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 510.217468\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 523.281616\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 527.405029\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 535.839783\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 498.897308\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 501.884308\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 520.258606\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 517.725403\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 524.208435\n",
            "\n",
            "Test set: Average loss: 0.0682, Reconstruction error: 0.1314467042684555\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 510.875519\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 516.397888\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 495.678741\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 523.868469\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 510.434662\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 501.687225\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 509.244476\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 526.640076\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 503.371765\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 518.430969\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 501.172577\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 523.312195\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 517.934326\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 504.320892\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 519.150757\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 516.470032\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 524.355286\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 503.835999\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 523.105042\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 490.564606\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 496.846436\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 511.928101\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 514.016357\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 505.475189\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 491.303833\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 511.400726\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 514.089905\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 514.865662\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 538.768799\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 502.209473\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 508.687408\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 533.002563\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 519.846008\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 526.183228\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 534.083069\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 530.259460\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 503.034698\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 525.958191\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 533.261536\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 518.750427\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 505.792236\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 507.832153\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 525.678894\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 521.147339\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 519.349670\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 525.622437\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 520.877991\n",
            "\n",
            "Test set: Average loss: 0.0686, Reconstruction error: 0.1313762664794922\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 501.586700\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 549.869690\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 536.544861\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 498.255920\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 510.160553\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 509.695282\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 533.623840\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 516.640503\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 517.749756\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 533.976929\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 530.572754\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 502.158325\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 526.031311\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 530.362549\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 515.656189\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 524.226196\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 525.548706\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 515.097351\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 516.357361\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 514.793762\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 528.169006\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 488.302979\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 517.768188\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 517.708618\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 523.879883\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 523.381348\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 513.409302\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 515.123962\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 506.931519\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 531.862427\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 523.424988\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 537.476379\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 525.885803\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 521.547668\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 524.656921\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 520.120544\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 530.467468\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 508.052429\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 505.721405\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 521.621643\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 511.700165\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 506.424591\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 518.718018\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 527.304749\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 495.699707\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 527.951538\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 513.068298\n",
            "\n",
            "Test set: Average loss: 0.0692, Reconstruction error: 0.1315169781446457\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 548.487854\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 531.810364\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 517.940430\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 511.826965\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 535.928955\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 544.561523\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 517.065857\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 525.554932\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 508.551605\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 541.247375\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 523.864685\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 508.800598\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 519.177917\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 529.649353\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 538.077820\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 512.312073\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 516.093384\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 528.696594\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 538.915100\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 535.663513\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 538.251770\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 529.749817\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 521.256653\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 540.833740\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 508.723358\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 514.450562\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 531.541077\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 514.176270\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 509.474884\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 529.395691\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 533.685913\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 523.871582\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 531.555298\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 525.308105\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 523.453491\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 534.838318\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 565.062195\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 531.895813\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 521.951355\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 540.099548\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 515.681152\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 497.458771\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 505.287231\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 521.043701\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 505.391052\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 524.074341\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 503.399689\n",
            "\n",
            "Test set: Average loss: 0.0698, Reconstruction error: 0.13148513436317444\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 533.516846\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 520.747070\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 528.524719\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 518.522583\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 533.277100\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 533.486511\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 544.411682\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 544.945740\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 520.851624\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 555.395630\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 528.230835\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 510.360321\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 521.071106\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 519.129700\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 534.480591\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 518.458740\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 528.463318\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 531.620728\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 559.625183\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 541.028442\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 512.611572\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 528.843567\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 537.504944\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 489.687836\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 523.091858\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 555.840027\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 507.062073\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 527.122131\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 529.505981\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 517.084656\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 524.591003\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 528.940125\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 534.022461\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 545.191406\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 541.528381\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 535.009827\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 511.474304\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 507.734314\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 524.376099\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 522.484741\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 514.103455\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 523.177246\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 541.123596\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 538.960999\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 528.945251\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 540.757751\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 519.253967\n",
            "\n",
            "Test set: Average loss: 0.0703, Reconstruction error: 0.13148713111877441\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 524.614136\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 542.115540\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 510.109772\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 520.139709\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 529.679199\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 536.091003\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 543.170776\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 545.013855\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 553.490234\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 511.816345\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 536.751404\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 529.977539\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 562.775757\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 528.760986\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 537.721863\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 530.466980\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 531.676819\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 540.792725\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 524.165161\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 541.131287\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 522.159119\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 550.592224\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 526.273438\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 545.129089\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 513.146973\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 546.443481\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 539.723389\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 545.085022\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 524.571350\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 541.183289\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 532.789734\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 538.264771\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 558.560791\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 552.872986\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 525.307922\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 511.483307\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 520.558105\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 514.853821\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 550.246338\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 520.804504\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 544.881287\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 525.301758\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 542.950012\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 527.815308\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 539.902771\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 507.163483\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 523.803589\n",
            "\n",
            "Test set: Average loss: 0.0707, Reconstruction error: 0.13144470751285553\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 538.733765\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 508.323151\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 529.728027\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 515.158691\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 522.454285\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 531.273743\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 529.498657\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 521.307434\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 552.768555\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 552.971313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Hpkmer3wEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "69e1bae1-ff84-43f5-d87d-df2630568117"
      },
      "source": [
        "sample = Beta(torch.tensor([1.0]), torch.tensor([5.0])).rsample([64,50]).squeeze().to(device)\n",
        "nums = np.random.randint(10, size=64)\n",
        "onehot = torch.eye(10, device=device)[nums]\n",
        "\n",
        "cumprods = torch.cat((torch.ones([64, 1], device=device), torch.cumprod(1-sample, axis=1)), dim=1)\n",
        "sample = cumprods[:,:-1]*sample\n",
        "sample[:, -1] = 1- sample[:, :-1].sum(axis=1)\n",
        "print(nums)\n",
        "inputs = torch.cat((sample, onehot), dim=1)\n",
        "sample = torch.sigmoid(sssbvae.decode(inputs)).reshape(64, 28, 28).cpu().detach().numpy()\n",
        "f, axarr = plt.subplots(8,8)\n",
        "for i in range(64):\n",
        "  axarr[i//8,i%8].imshow(sample[i])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 4 2 4 6 2 8 5 2 2 0 8 0 5 1 4 1 2 1 6 4 5 8 0 3 6 7 9 0 4 5 6 4 9 1 5 1\n",
            " 0 2 2 8 6 6 2 6 6 5 3 9 8 8 9 0 4 3 9 0 4 4 5 5 6 7 8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAD8CAYAAABaZT40AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 64 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfmG5q9g0ggp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /data/runs/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}